{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# RILA EDA: Feature Engineering - REFACTORED\n",
    "\n",
    "**Refactored:** 2026-01-28  \n",
    "**Original:** notebooks/rila/03_EDA_RILA_feature_engineering.ipynb  \n",
    "\n",
    "**Changes:**\n",
    "- Migrated from helpers.* to src.* imports\n",
    "- Added canonical sys.path auto-detection\n",
    "- Improved cell structure and documentation\n",
    "- Preserved exploratory flexibility\n",
    "- Added validation checkpoints\n",
    "\n",
    "**Purpose:** Exploratory feature engineering combining sales and competitive rate data. Creates CPI-adjusted features, weekly aggregations, lag features, and spreads for correlation analysis.\n",
    "\n",
    "**Dependencies:** Notebooks 1 and 2 (sales and rates data outputs)\n",
    "\n",
    "**Note:** EDA notebook - mathematical equivalence not required, exploratory flexibility preserved\n",
    "\n",
    "## Table of Contents\n",
    "* [Section 1: Load Preprocessed Data](#sec1:Load)\n",
    "* [Section 2: CPI Adjustment and Data Integration](#sec2:cpi)\n",
    "* [Section 3: Weekly Aggregation and Lag Features](#sec3:weekly)\n",
    "* [Section 4: Spread Calculation and Visualization](#sec4:spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture",
    "# =============================================================================",
    "# STANDARD SETUP CELL - Clean Dependency Pattern",
    "# =============================================================================",
    "",
    "# Standard library imports",
    "import sys",
    "import os",
    "from pathlib import Path",
    "import pandas as pd",
    "import numpy as np",
    "import warnings",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "from datetime import datetime, timedelta",
    "",
    "# Suppress warnings for clean output",
    "warnings.filterwarnings(\"ignore\")",
    "",
    "# Canonical sys.path setup (auto-detect project root)",
    "# Canonical sys.path setup (auto-detect project root)\n",
    "# Auto-detect project root (handles actual directory structure)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Check for actual directory structure\n",
    "if 'notebooks/production/rila' in cwd:\n",
    "    project_root = Path(cwd).parents[2]\n",
    "elif 'notebooks/production/fia' in cwd:\n",
    "    project_root = Path(cwd).parents[2]\n",
    "elif 'notebooks/eda/rila' in cwd:\n",
    "    project_root = Path(cwd).parents[2]\n",
    "elif 'notebooks/archive' in cwd:\n",
    "    project_root = Path(cwd).parents[2]\n",
    "elif os.path.basename(cwd) == 'notebooks':\n",
    "    project_root = Path(cwd).parent\n",
    "else:\n",
    "    project_root = Path(cwd)\n",
    "\n",
    "project_root = str(project_root)\n",
    "\n",
    "# IMPORTANT: Verify import will work\n",
    "if not os.path.exists(os.path.join(project_root, 'src')):\n",
    "    raise RuntimeError(\n",
    "        f\"sys.path setup failed: 'src' package not found at {project_root}/src\\n\"\n",
    "        f\"Current directory: {cwd}\\n\"\n",
    "        \"This indicates the sys.path detection logic needs adjustment.\"\n",
    "    )\n",
    "\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "",
    "# Refactored imports (src.* pattern)",
    "from src.data import extraction as ext",
    "from src.data import pipelines",
    "from src.data.dvc_manager import save_dataset, load_dataset",
    "",
    "# Visualization theme",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")",
    "",
    "print(\"✓ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AWS CONFIGURATION - Reuse from 00_data_pipeline pattern\n",
    "# =============================================================================\n",
    "\n",
    "aws_config = {\n",
    "    'xid': \"x259830\",\n",
    "    'role_arn': \"arn:aws:iam::159058241883:role/isg-usbie-annuity-CA-s3-sharing\",\n",
    "    'sts_endpoint_url': \"https://sts.us-east-1.amazonaws.com\",\n",
    "    'source_bucket_name': \"pruvpcaws031-east-isg-ie-lake\",\n",
    "    'output_bucket_name': \"cdo-annuity-364524684987-bucket\",\n",
    "    'output_base_path': \"ANN_Price_Elasticity_Data_Science\"\n",
    "}\n",
    "\n",
    "# Product parameters\n",
    "version = \"v2_0\"\n",
    "\n",
    "# Date parameters\n",
    "current_time = datetime.now()\n",
    "current_date = current_time.strftime(\"%Y-%m-%d\")\n",
    "current_date_of_mature_data = (current_time - timedelta(days=0)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Analysis parameters\n",
    "start_date = \"2021-01-01\"\n",
    "end_date = current_time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Version: {version}\")\n",
    "print(f\"  Analysis period: {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-header",
   "metadata": {},
   "source": [
    "## Section 1: Load Preprocessed Data <a id=\"sec1:Load\"></a>\n",
    "\n",
    "**Business Purpose**: Load preprocessed sales, rates, and economic indicator data from previous EDA notebooks and production pipeline outputs\n",
    "\n",
    "**Data Sources**: \n",
    "- WINK competitive rates (from notebook 2)\n",
    "- Sales time series (from notebook 1)\n",
    "- Economic indicators (DGS5, VIX, CPI) from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Define file paths\n",
    "file_name_dgs5 = \"MACRO_ECONOMIC_DATA/DGS5_index\"\n",
    "file_name_vixcls = \"MACRO_ECONOMIC_DATA/VIXCLS_index\"\n",
    "file_name_cpi = \"MACRO_ECONOMIC_DATA/cpi_scaled\"\n",
    "file_name_sales = f\"RILA_{version}/FlexGuard_Sales\"\n",
    "file_name_contract = f\"RILA_{version}/FlexGuard_Sales_contract\"\n",
    "\n",
    "# Load economic data from S3\n",
    "df_dgs5 = ext.download_s3_parquet_with_optional_date_suffix(\n",
    "    aws_config['output_bucket_name'], \n",
    "    f\"{aws_config['output_base_path']}/{file_name_dgs5}\", \n",
    "    None\n",
    ")\n",
    "df_vixcls = ext.download_s3_parquet_with_optional_date_suffix(\n",
    "    aws_config['output_bucket_name'], \n",
    "    f\"{aws_config['output_base_path']}/{file_name_vixcls}\", \n",
    "    None\n",
    ")\n",
    "df_cpi = ext.download_s3_parquet_with_optional_date_suffix(\n",
    "    aws_config['output_bucket_name'], \n",
    "    f\"{aws_config['output_base_path']}/{file_name_cpi}\", \n",
    "    None\n",
    ")\n",
    "\n",
    "# Load sales data from S3\n",
    "df_sales = ext.download_s3_parquet_with_optional_date_suffix(\n",
    "    aws_config['output_bucket_name'], \n",
    "    f\"{aws_config['output_base_path']}/{file_name_sales}\", \n",
    "    current_date\n",
    ")\n",
    "df_sales_contract = ext.download_s3_parquet_with_optional_date_suffix(\n",
    "    aws_config['output_bucket_name'], \n",
    "    f\"{aws_config['output_base_path']}/{file_name_contract}\", \n",
    "    current_date\n",
    ")\n",
    "\n",
    "# Load rates from DVC-tracked EDA output (from notebook 2)\n",
    "try:\n",
    "    df_rates = load_dataset(\"WINK_competitive_landscape_1Y10_EDA\")\n",
    "except:\n",
    "    # Fallback to S3 if DVC not available\n",
    "    print(\"  Note: Loading from S3 fallback (DVC not available)\")\n",
    "    date_path = f\"year={current_time.year}/month={current_time.month:02}/day={current_time.day:02}\"\n",
    "    file_path = f\"WINK_rate_features_archive/RILA_{version}/{date_path}\"\n",
    "    file_name = f\"WINK_competitive_landscape_1Y10\"\n",
    "    df_rates = ext.download_s3_parquet_with_optional_date_suffix(\n",
    "        aws_config['output_bucket_name'], \n",
    "        f\"{aws_config['output_base_path']}/{file_path}/{file_name}\", \n",
    "        None\n",
    "    )\n",
    "\n",
    "print(f\"✓ All data loaded\")\n",
    "print(f\"  Rates: {df_rates.shape}\")\n",
    "print(f\"  Sales: {df_sales.shape}\")\n",
    "print(f\"  Economic indicators: DGS5({df_dgs5.shape}), VIX({df_vixcls.shape}), CPI({df_cpi.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-checkpoint-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION CHECKPOINT\n",
    "# =============================================================================\n",
    "\n",
    "# Validate data quality\n",
    "assert not df_rates.empty, \"Rates DataFrame is empty\"\n",
    "assert not df_sales.empty, \"Sales DataFrame is empty\"\n",
    "assert 'date' in df_rates.columns, \"Missing date column in rates\"\n",
    "assert 'date' in df_sales.columns, \"Missing date column in sales\"\n",
    "\n",
    "print(f\"✓ Data validation passed\")\n",
    "print(f\"  Rates records: {len(df_rates):,}\")\n",
    "print(f\"  Sales records: {len(df_sales):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-header",
   "metadata": {},
   "source": [
    "## Section 2: CPI Adjustment and Data Integration <a id=\"sec2:cpi\"></a>\n",
    "\n",
    "Integrate sales, rates, and economic indicators with CPI adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA INTEGRATION - Using refactored pipeline\n",
    "# =============================================================================\n",
    "\n",
    "# Configure data integration (following 00_data_pipeline pattern)\n",
    "data_sources = {\n",
    "    'sales': df_sales,\n",
    "    'sales_contract': df_sales_contract,\n",
    "    'dgs5': df_dgs5,\n",
    "    'vixcls': df_vixcls,\n",
    "    'cpi': df_cpi\n",
    "}\n",
    "\n",
    "data_integration_config = {\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date,\n",
    "    'merge_how': 'outer',\n",
    "    'fill_method': 'ffill'\n",
    "}\n",
    "\n",
    "# Apply data integration (creates CPI-adjusted sales)\n",
    "df_sales_cpi_adj = pipelines.apply_data_integration(\n",
    "    df_rates, \n",
    "    data_sources, \n",
    "    data_integration_config\n",
    ")\n",
    "\n",
    "# Merge with rates\n",
    "df_ts = df_rates.merge(df_sales_cpi_adj, on=\"date\").drop_duplicates()\n",
    "\n",
    "print(f\"✓ Data integration complete\")\n",
    "print(f\"  Integrated dataset: {df_ts.shape}\")\n",
    "print(f\"  Date range: {df_ts['date'].min()} to {df_ts['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-checkpoint-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION CHECKPOINT\n",
    "# =============================================================================\n",
    "\n",
    "assert not df_ts.empty, \"Integrated DataFrame is empty\"\n",
    "assert df_ts.shape[0] > 100, f\"Expected >100 rows, got {df_ts.shape[0]}\"\n",
    "assert 'date' in df_ts.columns, \"Missing date column\"\n",
    "\n",
    "print(f\"✓ Integration validation passed\")\n",
    "print(f\"  Integrated records: {len(df_ts):,}\")\n",
    "print(f\"  Columns: {df_ts.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-header",
   "metadata": {},
   "source": [
    "## Section 3: Weekly Aggregation and Lag Features <a id=\"sec3:weekly\"></a>\n",
    "\n",
    "Aggregate to weekly frequency and create lag features for time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-aggregation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WEEKLY AGGREGATION - Using refactored pipeline\n",
    "# =============================================================================\n",
    "\n",
    "# Configure weekly aggregation\n",
    "weekly_agg_config = {\n",
    "    'freq': 'W',\n",
    "    'how': 'last',  # Use last value in week\n",
    "    'rolling': 1,   # No additional smoothing\n",
    "    'numeric_agg': 'mean'  # For numeric columns\n",
    "}\n",
    "\n",
    "# Apply weekly aggregation\n",
    "df_ts_weekly = pipelines.apply_weekly_aggregation(df_ts, weekly_agg_config)\n",
    "\n",
    "print(f\"✓ Weekly aggregation complete\")\n",
    "print(f\"  Weekly records: {len(df_ts_weekly):,}\")\n",
    "print(f\"  Date range: {df_ts_weekly['date'].min()} to {df_ts_weekly['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lag-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LAG FEATURES - Using refactored pipeline\n",
    "# =============================================================================\n",
    "\n",
    "# Configure lag features (following 00_data_pipeline pattern)\n",
    "lag_config = {\n",
    "    'lag_periods': [0, 1, 2, 4, 7],  # Common lags for EDA\n",
    "    'create_polynomials': False,  # Keep simple for EDA\n",
    "    'semantic_naming': True\n",
    "}\n",
    "\n",
    "# Apply lag features\n",
    "df_ts_weekly_w_lag = pipelines.apply_lag_and_polynomial_features(\n",
    "    df_ts_weekly, \n",
    "    lag_config\n",
    ")\n",
    "\n",
    "# Filter to analysis period\n",
    "mask_time = df_ts_weekly_w_lag[\"date\"] > pd.to_datetime(\"2021-02-01\")\n",
    "df_weekly = df_ts_weekly_w_lag[mask_time]\n",
    "df_weekly = df_weekly[sorted(df_weekly.columns)].reset_index(drop=True)\n",
    "\n",
    "# Add temporal features\n",
    "df_weekly[\"day_of_year\"] = df_weekly.date.dt.day_of_year\n",
    "mask_holiday = (df_weekly[\"day_of_year\"] < 13) | (df_weekly[\"day_of_year\"] > 359)\n",
    "df_weekly[\"holiday\"] = mask_holiday.astype(\"int\")\n",
    "\n",
    "print(f\"✓ Lag features created\")\n",
    "print(f\"  Final records: {len(df_weekly):,}\")\n",
    "print(f\"  Total features: {df_weekly.shape[1]}\")\n",
    "print(f\"  Date range: {df_weekly['date'].min()} to {df_weekly['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-checkpoint-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VALIDATION CHECKPOINT\n",
    "# =============================================================================\n",
    "\n",
    "assert not df_weekly.empty, \"Weekly DataFrame is empty\"\n",
    "assert df_weekly.shape[0] > 50, f\"Expected >50 weekly records, got {df_weekly.shape[0]}\"\n",
    "assert 'date' in df_weekly.columns, \"Missing date column\"\n",
    "assert 'holiday' in df_weekly.columns, \"Missing holiday feature\"\n",
    "\n",
    "print(f\"✓ Weekly data validation passed\")\n",
    "print(f\"  Weekly records: {len(df_weekly):,}\")\n",
    "print(f\"  Features: {df_weekly.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE ENGINEERED FEATURES - Using DVC for tracking\n",
    "# =============================================================================\n",
    "\n",
    "# Save for downstream notebooks\n",
    "save_dataset(df_weekly, \"RILA_engineered_features_EDA\")\n",
    "\n",
    "print(f\"✓ Engineered features saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4-header",
   "metadata": {},
   "source": [
    "## Section 4: Spread Calculation and Visualization <a id=\"sec4:spread\"></a>\n",
    "\n",
    "Calculate competitive spread and visualize relationship with sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SPREAD CALCULATION (exploratory - multiple variants)\n",
    "# =============================================================================\n",
    "\n",
    "# Filter to non-holiday periods\n",
    "mask_time = df_weekly[\"date\"] <= current_date_of_mature_data\n",
    "df_weekly_filtered = df_weekly[df_weekly[\"holiday\"] == 0].copy()\n",
    "\n",
    "# Calculate different spread variants (exploratory)\n",
    "if 'P_lag_0' in df_weekly_filtered.columns and 'C_lag_0' in df_weekly_filtered.columns:\n",
    "    df_weekly_filtered[\"Spread\"] = df_weekly_filtered[\"P_lag_0\"] - df_weekly_filtered[\"C_lag_0\"]\n",
    "    df_weekly_filtered[\"Spread_2\"] = df_weekly_filtered[\"P_lag_0\"] - df_weekly_filtered[\"C_lag_0\"]\n",
    "    \n",
    "    print(f\"✓ Spread calculated\")\n",
    "    print(f\"  Spread range: {df_weekly_filtered['Spread'].min():.2f} to {df_weekly_filtered['Spread'].max():.2f}\")\n",
    "else:\n",
    "    print(\"  Warning: Lag columns not found, using alternative approach\")\n",
    "    # Fallback: calculate spread from raw columns\n",
    "    if 'Prudential' in df_weekly_filtered.columns and 'C_weighted_mean' in df_weekly_filtered.columns:\n",
    "        df_weekly_filtered[\"Spread\"] = df_weekly_filtered[\"Prudential\"] - df_weekly_filtered[\"C_weighted_mean\"]\n",
    "        print(f\"✓ Spread calculated (fallback method)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRELATION VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Determine sales column (flexible for different data versions)\n",
    "sales_col = None\n",
    "for col in [\"sales_forward_0\", \"sales\", \"sales_target_contract_current\"]:\n",
    "    if col in df_weekly_filtered.columns:\n",
    "        sales_col = col\n",
    "        break\n",
    "\n",
    "if sales_col and 'Spread' in df_weekly_filtered.columns:\n",
    "    figure, axes = plt.subplots(2, 1, sharex=False, sharey=False, figsize=(12, 10))\n",
    "    \n",
    "    figure.suptitle(f\"FLEXGUARD Feature Engineering EDA\")\n",
    "    axes[0].set_title(\n",
    "        \"CPI Adjusted FlexGuard Sales (Black) and Cap Rate Distance to Mean Rate (Orange)\"\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Date\")\n",
    "    axes[0].set_ylabel(\"CPI Adjusted FlexGuard Sales (dollars)\")\n",
    "    \n",
    "    # Plot sales\n",
    "    sns.lineplot(\n",
    "        df_weekly_filtered[:-1],\n",
    "        x=\"date\",\n",
    "        y=sales_col,\n",
    "        ax=axes[0],\n",
    "        color=\"k\",\n",
    "        linewidth=5,\n",
    "        errorbar=(\"pi\", 95),\n",
    "    )\n",
    "    \n",
    "    # Plot spread on secondary axis\n",
    "    ax0 = axes[0].twinx()\n",
    "    ax0.set_ylabel(\"Cap Rate Distance to Mean Rate (bps)\")\n",
    "    ax0.grid(False)\n",
    "    sns.lineplot(\n",
    "        data=df_weekly_filtered, \n",
    "        x=\"date\", \n",
    "        y=\"Spread\", \n",
    "        ax=ax0, \n",
    "        color=\"tab:orange\", \n",
    "        linewidth=5\n",
    "    )\n",
    "    \n",
    "    # Scatter plot\n",
    "    sns.regplot(\n",
    "        df_weekly_filtered, \n",
    "        x=\"Spread\", \n",
    "        y=sales_col, \n",
    "        ax=axes[1], \n",
    "        ci=100, \n",
    "        order=1\n",
    "    )\n",
    "    \n",
    "    axes[1].set_title(\"CPI Adjusted FlexGuard Sales vs. Cap Rate Distance to Mean Rate\")\n",
    "    axes[1].set_xlabel(\"Cap Rate Distance to Mean Rate (bps)\")\n",
    "    axes[1].set_ylabel(\"CPI Adjusted FlexGuard Sales (dollars)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_engineering_correlation.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = df_weekly_filtered[sales_col].corr(df_weekly_filtered[\"Spread\"])\n",
    "    print(f\"\\n✓ Visualization complete\")\n",
    "    print(f\"  Correlation (Sales vs Spread): {correlation:.3f}\")\n",
    "    print(f\"  Saved to: feature_engineering_correlation.png\")\n",
    "else:\n",
    "    print(\"  Warning: Required columns not found for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completion-cell",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## EDA Complete\n",
    "\n",
    "**Key Findings:**\n",
    "- Sales and competitive rates integrated with economic indicators\n",
    "- CPI-adjusted features created\n",
    "- Weekly aggregation reduces noise while preserving trends\n",
    "- Lag features capture temporal dependencies\n",
    "- Spread (Prudential - Competitor) shows correlation with sales\n",
    "\n",
    "**Engineered Features:**\n",
    "- CPI-adjusted sales\n",
    "- Weekly aggregated time series\n",
    "- Lag features (0, 1, 2, 4, 7 periods)\n",
    "- Holiday indicators\n",
    "- Competitive spread\n",
    "\n",
    "**Next Steps:** \n",
    "- Proceed to 04_RILA_feature_selection.ipynb for model-based feature selection\n",
    "- Use engineered features for AIC-based selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}