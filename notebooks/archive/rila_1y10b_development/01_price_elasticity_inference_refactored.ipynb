{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RILA 1Y10B Price Elasticity Inference - Refactored\n",
    "\n",
    "**Product**: FlexGuard 1Y10B (1-year term, 10% buffer)\n",
    "\n",
    "**Architecture**: Uses `UnifiedNotebookInterface` for inference with methodology validation\n",
    "\n",
    "**Created**: 2026-01-26\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Run price elasticity inference using the refactored API:\n",
    "- **Data Source**: Processed data from `00_data_pipeline.ipynb`\n",
    "- **Inference**: Uses `UnifiedNotebookInterface` with RILA methodology\n",
    "- **Validation**: Economic constraint checking (own rate positive, competitor negative)\n",
    "\n",
    "**Key Benefits**:\n",
    "1. Methodology-aware inference (RILA constraint rules)\n",
    "2. Automatic lag-0 competitor validation\n",
    "3. Environment-agnostic (same code works for all products)\n",
    "4. Cleaner separation between data processing and inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to path\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-detect project root\n",
    "project_root = Path().resolve()\n",
    "while not (project_root / \"src\").exists() and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Our refactored interface\n",
    "from src.notebooks import create_interface\n",
    "\n",
    "# Legacy imports for visualization (not yet refactored)\n",
    "from src.visualization.inference_plots import (\n",
    "    prepare_visualization_data_pct,\n",
    "    prepare_visualization_data_dollars,\n",
    "    generate_price_elasticity_visualization_pct,\n",
    "    generate_price_elasticity_visualization_dollars,\n",
    "    save_visualization_files,\n",
    "    export_csv_files\n",
    ")\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Reproducibility Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REPRODUCIBILITY: Random Seed Initialization\n",
    "# =============================================================================\n",
    "RANDOM_SEED = 42  # Fixed seed for reproducible bootstrap results\n",
    "\n",
    "# Set all random seeds for full reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"✓ Random seed initialized: {RANDOM_SEED}\")\n",
    "print(\"  All bootstrap operations will be reproducible across runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Create Interface and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interface for 1Y10B product\n",
    "# Note: We use \"local\" environment since we're loading from processed parquet files\n",
    "interface = create_interface(\n",
    "    product_code=\"1Y10B\",\n",
    "    environment=\"local\",\n",
    "    adapter_kwargs={\"data_dir\": project_root / \"notebooks/rila_1y10b/outputs/datasets_1y10b\"}\n",
    ")\n",
    "\n",
    "# Verify configuration\n",
    "print(\"Product Configuration:\")\n",
    "print(f\"  Code: {interface.product.product_code}\")\n",
    "print(f\"  Name: {interface.product.name}\")\n",
    "print(f\"  Type: {interface.product.product_type}\")\n",
    "print(f\"  Buffer: {interface.product.buffer_level * 100:.0f}%\")\n",
    "print(f\"  Term: {interface.product.term_years} year(s)\")\n",
    "print()\n",
    "print(f\"Methodology: {interface.methodology}\")\n",
    "print(f\"Constraint rules active: {len(interface.get_constraint_rules())} rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from data pipeline notebook\n",
    "data_path = project_root / \"notebooks/rila_1y10b/outputs/datasets_1y10b/final_dataset.parquet\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data file not found: {data_path}\\n\"\n",
    "        \"Please run notebooks/rila_1y10b/00_data_pipeline.ipynb first.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "print(f\"Loaded dataset: {df.shape[0]:,} rows × {df.shape[1]:,} columns\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Filtering and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply business filters\n",
    "# 1. Remove zero sales records\n",
    "initial_count = len(df)\n",
    "df_filtered = df[df[\"sales\"] != 0].copy()\n",
    "print(f\"Zero sales filter: {initial_count:,} → {len(df_filtered):,} records\")\n",
    "\n",
    "# 2. Create temporal weight decay for model training\n",
    "weight_decay_factor = 0.999\n",
    "df_filtered[\"weight\"] = [weight_decay_factor ** (len(df_filtered) - k) for k in range(len(df_filtered))]\n",
    "\n",
    "# 3. Apply date filter (use data from 2022-09-01 onwards)\n",
    "date_filter = pd.to_datetime(\"2022-09-01\")\n",
    "mask_time = df_filtered[\"date\"] > date_filter\n",
    "df_model = df_filtered[mask_time][:-1].copy()  # Exclude last record to prevent lookahead bias\n",
    "\n",
    "print(f\"\\nFinal training dataset: {len(df_model):,} records\")\n",
    "print(f\"Date range: {df_model['date'].min()} to {df_model['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Feature Selection and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define features for inference\n# These are the key drivers of RILA sales elasticity\n# Note: Using features available in fixture data\nfeatures = [\n    # Own rate (treatment variable) - Prudential's cap rate\n    \"prudential_rate_current\",\n    \n    # Competitor rates (lagged to avoid simultaneity bias)\n    \"competitor_mid_t2\",  # t-2 lag\n    \"competitor_mid_t3\",  # t-3 lag\n    \"competitor_mid_t4\",  # t-4 lag\n    \n    # Economic indicators\n    \"econ_treasury_5y_t1\",  # 5-year treasury rate\n    \"VIXCLS\",  # VIX volatility index\n]\n\ntarget = \"sales_target_current\"\n\nprint(\"Model Configuration:\")\nprint(f\"  Target: {target}\")\nprint(f\"  Features: {len(features)}\")\nfor i, feat in enumerate(features, 1):\n    print(f\"    {i}. {feat}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate features using interface methodology\n",
    "# This checks for lag-0 competitor features (forbidden by causal framework)\n",
    "print(\"Validating features against RILA methodology:\\n\")\n",
    "\n",
    "for feature in features:\n",
    "    is_forbidden = interface._is_competitor_lag_zero(feature)\n",
    "    status = \"❌ FORBIDDEN\" if is_forbidden else \"✓ OK\"\n",
    "    print(f\"  {feature}: {status}\")\n",
    "\n",
    "# Check if any features violate constraints\n",
    "forbidden_features = [f for f in features if interface._is_competitor_lag_zero(f)]\n",
    "if forbidden_features:\n",
    "    raise ValueError(\n",
    "        f\"Lag-0 competitor features detected: {forbidden_features}\\n\"\n",
    "        \"These violate causal identification (simultaneity bias).\\n\"\n",
    "        \"Use lagged competitors (t1, t2, t3...) instead.\"\n",
    "    )\n",
    "\n",
    "print(\"\\n✓ All features validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Run Inference\n",
    "\n",
    "**Note**: As documented in `CURRENT_WORK.md`, `interface.run_inference()` has limitations (returns hardcoded zeros). \n",
    "\n",
    "For production use, we use the legacy `center_baseline()` function directly until the interface is fully implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import legacy inference function (workaround for interface limitation)\n",
    "from src.models.inference import center_baseline\n",
    "\n",
    "# Prepare data for inference\n",
    "X = df_model[features]\n",
    "y = df_model[target]\n",
    "weights = df_model[\"weight\"]\n",
    "\n",
    "# Run bootstrap ensemble inference\n",
    "print(\"Running bootstrap ensemble inference...\")\n",
    "print(f\"  Observations: {len(X):,}\")\n",
    "print(f\"  Features: {len(features)}\")\n",
    "print(f\"  Bootstrap samples: 1000\")\n",
    "print()\n",
    "\n",
    "results = center_baseline(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    n_estimators=1000,\n",
    "    sample_weight=weights,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"Inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Coefficient Validation\n",
    "\n",
    "Use the interface's methodology to validate coefficients meet economic constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients from results\n",
    "coefficients = dict(zip(features, results['coefficients']))\n",
    "\n",
    "# Display coefficients\n",
    "print(\"Model Coefficients:\")\n",
    "print(\"=\" * 60)\n",
    "for feature, coef in coefficients.items():\n",
    "    sign = \"+\" if coef > 0 else \"\"\n",
    "    print(f\"  {feature:30s}: {sign}{coef:10.6f}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate against RILA methodology constraints\n",
    "validation = interface.validate_coefficients(coefficients)\n",
    "\n",
    "print(\"Economic Constraint Validation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if validation[\"passed\"]:\n",
    "    print(\"\\n✓ PASSED Constraints:\")\n",
    "    for item in validation[\"passed\"]:\n",
    "        print(f\"  {item['feature']:30s}: {item['coefficient']:+.4f} (expected {item['expected']})\")\n",
    "\n",
    "if validation[\"violated\"]:\n",
    "    print(\"\\n❌ VIOLATED Constraints:\")\n",
    "    for item in validation[\"violated\"]:\n",
    "        print(f\"  {item['feature']:30s}: {item['coefficient']:+.4f}\")\n",
    "        print(f\"    Expected {item['expected']}, got {item['actual']}\")\n",
    "\n",
    "# Overall status\n",
    "print()\n",
    "if not validation[\"violated\"]:\n",
    "    print(\"✓ All economic constraints satisfied\")\n",
    "else:\n",
    "    print(f\"⚠ {len(validation['violated'])} constraint(s) violated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Model Fit Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model fit statistics\n",
    "print(\"Model Fit Metrics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  R² Score:          {results['r2']:.4f}\")\n",
    "print(f\"  Mean Squared Error: {results['mse']:.2f}\")\n",
    "print(f\"  Mean Absolute Error: {results['mae']:.2f}\")\n",
    "print()\n",
    "print(f\"  Training samples:   {len(X):,}\")\n",
    "print(f\"  Features used:      {len(features)}\")\n",
    "print(f\"  Bootstrap samples:  1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Rate Scenario Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rate scenarios for prediction\n",
    "from src.models.inference import rate_adjustments\n",
    "\n",
    "# Generate rate scenarios from -200 to +200 bps in 10 bps increments\n",
    "rate_scenarios = np.arange(-0.02, 0.021, 0.001)  # -2% to +2% in 0.1% steps\n",
    "\n",
    "print(f\"Running predictions for {len(rate_scenarios)} rate scenarios...\")\n",
    "\n",
    "# Get predictions for each scenario\n",
    "predictions_df = rate_adjustments(\n",
    "    model=results['model'],\n",
    "    X=X,\n",
    "    features=features,\n",
    "    rate_range=rate_scenarios,\n",
    "    baseline_sales=df_model['sales'].iloc[-1]\n",
    ")\n",
    "\n",
    "print(f\"Predictions generated for scenarios from {rate_scenarios[0]*100:.1f}% to {rate_scenarios[-1]*100:.1f}%\")\n",
    "print(f\"\\nPreview of predictions:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals for predictions\n",
    "from src.models.inference import confidence_interval\n",
    "\n",
    "print(\"Calculating bootstrap confidence intervals...\")\n",
    "\n",
    "ci_results = confidence_interval(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    features=features,\n",
    "    rate_scenarios=rate_scenarios,\n",
    "    n_bootstrap=100,  # Use 100 for speed; increase for production\n",
    "    confidence_level=0.95,\n",
    "    sample_weight=weights,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"Confidence intervals calculated\")\n",
    "print(f\"\\nSample confidence intervals (first 5 scenarios):\")\n",
    "for i in range(min(5, len(ci_results))):\n",
    "    scenario = rate_scenarios[i] * 100\n",
    "    lower = ci_results['lower'][i]\n",
    "    upper = ci_results['upper'][i]\n",
    "    mean = ci_results['mean'][i]\n",
    "    print(f\"  Scenario {scenario:+.1f}%: [{lower:,.0f}, {upper:,.0f}] (mean: {mean:,.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare visualization data\n",
    "viz_data_pct = prepare_visualization_data_pct(predictions_df, ci_results)\n",
    "viz_data_dollars = prepare_visualization_data_dollars(predictions_df, ci_results, df_model['sales'].iloc[-1])\n",
    "\n",
    "# Generate plots\n",
    "fig_pct = generate_price_elasticity_visualization_pct(viz_data_pct)\n",
    "fig_dollars = generate_price_elasticity_visualization_dollars(viz_data_dollars)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "output_dir = Path(\"outputs/inference_1y10b\")\n",
    "bi_dir = Path(\"BI_TEAM_1Y10B\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "bi_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save visualizations\n",
    "save_visualization_files(\n",
    "    fig_pct=fig_pct,\n",
    "    fig_dollars=fig_dollars,\n",
    "    output_dir=output_dir,\n",
    "    product_code=\"1Y10B\"\n",
    ")\n",
    "\n",
    "# Export CSV files for Tableau\n",
    "export_csv_files(\n",
    "    predictions_df=predictions_df,\n",
    "    ci_results=ci_results,\n",
    "    coefficients=coefficients,\n",
    "    output_dir=bi_dir,\n",
    "    product_code=\"1Y10B\"\n",
    ")\n",
    "\n",
    "print(f\"Results exported to:\")\n",
    "print(f\"  Visualizations: {output_dir}\")\n",
    "print(f\"  Tableau CSVs: {bi_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What This Notebook Does\n",
    "\n",
    "1. **Creates interface** for 1Y10B with RILA methodology\n",
    "2. **Loads processed data** from data pipeline notebook\n",
    "3. **Validates features** against causal framework (no lag-0 competitors)\n",
    "4. **Runs inference** using bootstrap ensemble\n",
    "5. **Validates coefficients** against economic constraints\n",
    "6. **Generates predictions** for rate scenarios\n",
    "7. **Exports results** for business intelligence\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "| Feature | Legacy Approach | Refactored Approach |\n",
    "|---------|----------------|---------------------|\n",
    "| Constraint validation | Manual | Automatic (methodology) |\n",
    "| Lag-0 detection | Manual checks | Built-in validation |\n",
    "| Product switching | Edit hardcoded values | Change product_code |\n",
    "| Economic rules | Scattered in code | Centralized in methodology |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run forecasting notebook: `02_time_series_forecasting_refactored.ipynb`\n",
    "2. Compare results with legacy inference notebook\n",
    "3. Validate mathematical equivalence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}