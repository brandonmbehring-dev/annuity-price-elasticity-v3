{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RILA Price Elasticity Analysis - Refactored Pipeline Architecture\n",
    "\n",
    "**Refactored:** 2025-10-27 - One Pipeline Per Cell Block\n",
    "**Original:** notebooks/00_data_pipeline.ipynb\n",
    "**Mathematical Equivalence:** Validated per Universal Refactoring Rules\n",
    "\n",
    "## Refactoring Improvements\n",
    "- **One Pipeline Per Cell**: Each of the 10 pipeline functions isolated in dedicated cells\n",
    "- **Enhanced Documentation**: Business purpose + technical specifications for each pipeline\n",
    "- **Improved DVC Integration**: Strategic checkpoints and optimized tracking\n",
    "- **Better Organization**: Clear separation of concerns and logical flow\n",
    "- **Atomic Refactoring Ready**: Each pipeline can be independently modified and validated\n",
    "\n",
    "## Data Flow Overview\n",
    "1. **Sales Pipeline**: Product filtering → cleanup → time series (app & contract dates)\n",
    "2. **WINK Pipeline**: Rate processing → market share weighting (separate for DRY)\n",
    "3. **Feature Pipeline**: Data integration → competitive features → weekly aggregation → lag engineering → final prep\n",
    "\n",
    "## Pipeline Functions (10 Total)\n",
    "1. `apply_product_filters()` - Standard annuity product filtering\n",
    "2. `apply_sales_data_cleanup()` - Comprehensive sales data preparation\n",
    "3. `apply_application_time_series()` - Application date time series\n",
    "4. `apply_contract_time_series()` - Contract date time series\n",
    "5. `apply_wink_rate_processing()` - WINK competitive rate processing (no C_weighted_mean)\n",
    "6. `apply_market_share_weighting()` - Market share weighting (creates C_weighted_mean)\n",
    "7. `apply_data_integration()` - Daily data integration without weekly aggregation\n",
    "8. `apply_competitive_features()` - Atomic competitive functions (clean semantic names)\n",
    "9. `apply_weekly_aggregation()` - Weekly aggregation after competitive features\n",
    "10. `apply_lag_and_polynomial_features()` - 13 lag configs + polynomial interactions\n",
    "11. `apply_final_feature_preparation()` - Temporal features, spreads, cleanup\n",
    "\n",
    "## Table of Contents\n",
    "* [Section 1: Configuration & Setup](#sec1:config)\n",
    "* [Section 2: Data Connection & Loading](#sec2:data_loading)\n",
    "* [Section 3: Sales Data Processing](#sec3:sales)\n",
    "* [Section 4: Competitive Rate Analysis](#sec4:rates)\n",
    "* [Section 5: Feature Engineering](#sec5:features)\n",
    "* [Section 6: Final Export & Visualization](#sec6:final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Section 1: Configuration & Setup <a id=\"sec1:config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:42.519601Z",
     "iopub.status.busy": "2026-01-19T16:39:42.519500Z",
     "iopub.status.idle": "2026-01-19T16:39:43.660515Z",
     "shell.execute_reply": "2026-01-19T16:39:43.659624Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# =============================================================================\n",
    "# UNIFIED DEPENDENCY PATTERN: Clean Organization\n",
    "# =============================================================================\n",
    "\n",
    "# Standard library imports (consistent across all notebooks)\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# FIXED: 2026-01-29 - Corrected sys.path detection for /notebooks/production/rila_* structure\n",
    "# OLD: Checked for 'notebooks/rila/production' (wrong - vestigial path from old structure)\n",
    "# NEW: Checks for 'notebooks/production/rila' (correct - actual directory structure)\n",
    "# See: docs/development/NOTEBOOK_SYS_PATH_FIX.md for details\n",
    "# =============================================================================\n",
    "\n",
    "# Canonical sys.path setup following MODULE_HIERARCHY.md\n",
    "# Auto-detect project root (handles actual directory structure)\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Check for actual production notebook structure\n",
    "if 'notebooks/production/rila' in cwd:\n",
    "    # Running from /notebooks/production/rila_6y20b/ or rila_1y10b/\n",
    "    project_root = Path(cwd).parents[2]\n",
    "elif 'notebooks/production/fia' in cwd:\n",
    "    # Running from /notebooks/production/fia/\n",
    "    project_root = Path(cwd).parents[2]\n",
    "elif 'notebooks/eda/rila' in cwd:\n",
    "    # Running from /notebooks/eda/rila_6y20b/\n",
    "    project_root = Path(cwd).parents[2]\n",
    "elif 'notebooks/archive' in cwd:\n",
    "    # Running from /notebooks/archive/*/\n",
    "    project_root = Path(cwd).parents[2]\n",
    "elif os.path.basename(cwd) == 'notebooks':\n",
    "    # Running from /notebooks/\n",
    "    project_root = Path(cwd).parent\n",
    "else:\n",
    "    # Running from project root (fallback)\n",
    "    project_root = Path(cwd)\n",
    "\n",
    "project_root = str(project_root)\n",
    "\n",
    "# IMPORTANT: Verify import will work (catches future sys.path issues early)\n",
    "if not os.path.exists(os.path.join(project_root, 'src')):\n",
    "    raise RuntimeError(\n",
    "        f\"sys.path setup failed: 'src' package not found at {project_root}/src\\n\"\n",
    "        f\"Current directory: {cwd}\\n\"\n",
    "        \"This indicates the sys.path detection logic needs adjustment for your directory structure.\"\n",
    "    )\n",
    "\n",
    "sys.path.insert(0, project_root)\n",
    "print(f\"✓ Project root: {project_root}\")\n",
    "\n",
    "# Clean imports with centralized error handling in modules\n",
    "from src.data import extraction as ext\n",
    "from src.data import preprocessing as prep\n",
    "from src.data import pipelines\n",
    "from src.data.dvc_manager import save_dataset, load_dataset, checkpoint_pipeline\n",
    "from src.data.schema_validator import validate_pipeline_input, validate_pipeline_output\n",
    "from src.features import engineering as eng\n",
    "from src.visualization import visualization as viz\n",
    "from src.config import config_builder\n",
    "\n",
    "# Load performance tracking extension\n",
    "\n",
    "# Apply consistent visualization theme\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "offline-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:43.662224Z",
     "iopub.status.busy": "2026-01-19T16:39:43.661980Z",
     "iopub.status.idle": "2026-01-19T16:39:43.666626Z",
     "shell.execute_reply": "2026-01-19T16:39:43.665918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "✓ ONLINE MODE ACTIVE\n",
      "  Data source: AWS S3 (live data)\n",
      "  AWS Account: 364524684987\n",
      "  AWS User: SageMaker\n",
      "======================================================================\n",
      "\n",
      "✓ Output destination: Local\n",
      "  Location: /home/sagemaker-user/RILA_6Y20B_refactored/outputs/datasets\n",
      "\n",
      "======================================================================\n",
      "MODE CONFIGURATION COMPLETE\n",
      "  Data source: AWS S3\n",
      "  Output destination: Local directory\n",
      "  See: docs/MODE_TOGGLE_GUIDE.md for details\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DUAL MODE CONFIGURATION\n",
    "# =============================================================================\n",
    "# Two independent toggles control data pipeline behavior:\n",
    "#\n",
    "# 1. DATA SOURCE: Where to read input data from\n",
    "#    OFFLINE_MODE = False (DEFAULT): Use AWS S3 (live data from Prudential)\n",
    "#    OFFLINE_MODE = True: Use local fixtures (development without AWS)\n",
    "#\n",
    "# 2. OUTPUT DESTINATION: Where to write pipeline outputs\n",
    "#    WRITE_TO_S3 = False (DEFAULT): Write to local outputs/ directory\n",
    "#    WRITE_TO_S3 = True: Write to S3 bucket (production)\n",
    "#\n",
    "# See docs/MODE_TOGGLE_GUIDE.md for complete documentation\n",
    "# =============================================================================\n",
    "\n",
    "OFFLINE_MODE = False  # False = AWS S3 data (default), False = Local fixtures\n",
    "WRITE_TO_S3 = False   # False = Local outputs/ (default), True = S3 bucket\n",
    "\n",
    "# Configure data source (online vs offline)\n",
    "if OFFLINE_MODE:\n",
    "    from src.validation_support.aws_mock_layer import (\n",
    "        setup_offline_environment,\n",
    "        verify_fixture_availability,\n",
    "        is_offline_mode\n",
    "    )\n",
    "\n",
    "    fixture_path = os.path.join(project_root, 'tests/fixtures/aws_complete/')\n",
    "\n",
    "    # Verify fixtures exist\n",
    "    if not verify_fixture_availability(fixture_path=fixture_path):\n",
    "        raise RuntimeError(\n",
    "            f\"OFFLINE MODE ERROR: Required fixtures not found in {fixture_path}\\n\"\n",
    "            \"Solutions:\\n\"\n",
    "            \"  1. Set OFFLINE_MODE=False to use AWS S3 data\\n\"\n",
    "            \"  2. Run fixture capture: python scripts/capture_aws_fixtures.py\\n\"\n",
    "            \"  3. See: docs/AWS_CAPTURE_INSTRUCTIONS.md\"\n",
    "        )\n",
    "\n",
    "    # Configure offline mode\n",
    "    setup_offline_environment(fixture_path=fixture_path)\n",
    "\n",
    "    # Verify mode is active\n",
    "    assert is_offline_mode(), \"Failed to enable offline mode\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"✓ OFFLINE MODE ACTIVE\")\n",
    "    print(\"  Data source: Local fixtures (no AWS calls)\")\n",
    "    print(f\"  Fixture path: {fixture_path}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    # Verify AWS credentials before proceeding\n",
    "    import boto3\n",
    "    try:\n",
    "        sts = boto3.client('sts')\n",
    "        identity = sts.get_caller_identity()\n",
    "        print(\"=\"*70)\n",
    "        print(\"✓ ONLINE MODE ACTIVE\")\n",
    "        print(\"  Data source: AWS S3 (live data)\")\n",
    "        print(f\"  AWS Account: {identity['Account']}\")\n",
    "        print(f\"  AWS User: {identity['Arn'].split('/')[-1]}\")\n",
    "        print(\"=\"*70)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"AWS credentials not configured: {e}\\n\"\n",
    "            \"Solutions:\\n\"\n",
    "            \"  1. Set OFFLINE_MODE=True to use local fixtures\\n\"\n",
    "            \"  2. Configure AWS credentials (SageMaker IAM role or aws configure)\\n\"\n",
    "            \"  3. Test connectivity: python scripts/test_aws_connectivity.py\"\n",
    "        )\n",
    "\n",
    "# Configure output destination (local vs S3)\n",
    "from src.data.output_manager import configure_output_mode, get_output_info\n",
    "\n",
    "if WRITE_TO_S3:\n",
    "    # S3 output mode - will be configured after aws_config is defined in next cell\n",
    "    print(\"\\n⚠ S3 output mode will be configured after aws_config is loaded\")\n",
    "    print(\"  Outputs will be written to S3 bucket\")\n",
    "else:\n",
    "    # Local output mode (default)\n",
    "    configure_output_mode(write_to_s3=False)\n",
    "    info = get_output_info()\n",
    "    print(f\"\\n✓ Output destination: {info['mode']}\")\n",
    "    print(f\"  Location: {info['full_path']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODE CONFIGURATION COMPLETE\")\n",
    "print(f\"  Data source: {'Local fixtures' if OFFLINE_MODE else 'AWS S3'}\")\n",
    "print(f\"  Output destination: {'S3 bucket' if WRITE_TO_S3 else 'Local directory'}\")\n",
    "print(\"  See: docs/MODE_TOGGLE_GUIDE.md for details\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:43.667972Z",
     "iopub.status.busy": "2026-01-19T16:39:43.667862Z",
     "iopub.status.idle": "2026-01-19T16:39:43.672281Z",
     "shell.execute_reply": "2026-01-19T16:39:43.671583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Configuration system loaded\n",
      "   Product: FlexGuard indexed variable annuity 6Y 20%\n",
      "   Buffer: 0.2, Term: 6 years\n",
      "   Version: 6\n",
      "   Pipeline configs: ['product_filter', 'sales_cleanup', 'time_series', 'wink_processing', 'weekly_aggregation', 'competitive', 'data_integration', 'lag_features', 'final_features'])\n",
      "   Feature analysis start: 2021-02-01\n"
     ]
    }
   ],
   "source": [
    "# Type hints for configuration\n",
    "from typing import Dict, List, Any\n",
    "# =============================================================================\n",
    "# PIPELINE CONFIGURATION - Standard Configuration for Baseline\n",
    "# =============================================================================\n",
    "\n",
    "# AWS Infrastructure Configuration (unchanged)\n",
    "aws_config: Dict[str, str] = {\n",
    "    'xid': \"x259830\",\n",
    "    'role_arn': \"arn:aws:iam::159058241883:role/isg-usbie-annuity-CA-s3-sharing\",\n",
    "    'sts_endpoint_url': \"https://sts.us-east-1.amazonaws.com\",\n",
    "    'source_bucket_name': \"pruvpcaws031-east-isg-ie-lake\",\n",
    "    'output_bucket_name': \"cdo-annuity-364524684987-bucket\",\n",
    "    'output_base_path': \"ANN_Price_Elasticity_Data_Science\"\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# Extended modeling period to 2021-02-01 for additional training data\n",
    "# Change date: 2025-01-21\n",
    "# Benefit: +48 weeks of historical data (~23% increase)\n",
    "# Captures: COVID recovery period (Q1 2021)\n",
    "# =============================================================================\n",
    "from src.config.config_builder import build_pipeline_configs\n",
    "configs: Dict[str, Any] = build_pipeline_configs(\n",
    "    version=6,\n",
    "    product_name=\"FlexGuard indexed variable annuity\",\n",
    "    term_filter=\"6Y\",\n",
    "    buffer_rate_filter=\"20%\",\n",
    "    feature_analysis_start_date=\"2021-02-01\"  # Extended from 2022-01-01\n",
    ")\n",
    "\n",
    "# Product parameters\n",
    "version = 6  # Pipeline version\n",
    "product_name = \"FlexGuard indexed variable annuity 6Y 20%\"\n",
    "buffer = 0.20\n",
    "term = 6\n",
    "\n",
    "# FlexGuard competitor product IDs (for market share weighting)\n",
    "flexguard_product_ids = {\n",
    "    \"Prudential\": [2979],\n",
    "    \"Allianz\": [2162, 3699],\n",
    "    \"Athene\": [3409],\n",
    "    \"Brighthouse\": [2319, 4149],\n",
    "    \"Equitable\": [3282],\n",
    "    \"Jackson\": [3351, 4491],\n",
    "    \"Lincoln\": [2358, 4058],\n",
    "    \"Symetra\": [3263],\n",
    "    \"Trans\": [3495],\n",
    "}\n",
    "\n",
    "# Visualization Parameters\n",
    "fig_width = 20\n",
    "fig_height = 6\n",
    "line_width = 3\n",
    "\n",
    "print(f\"SUCCESS: Configuration system loaded\")\n",
    "print(f\"   Product: {product_name}\")\n",
    "print(f\"   Buffer: {buffer}, Term: {term} years\")\n",
    "print(f\"   Version: {version}\")\n",
    "print(f\"   Pipeline configs: {list(configs.keys())})\")\n",
    "print(f\"   Feature analysis start: {configs['final_features']['feature_analysis_start_date']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "xujpeonue7n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local output mode (outputs/datasets/)\n",
      "To enable S3 outputs: Set WRITE_TO_S3=True in previous cell\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# S3 OUTPUT CONFIGURATION (if WRITE_TO_S3=True)\n",
    "# =============================================================================\n",
    "# Complete S3 output setup now that aws_config is available\n",
    "\n",
    "if WRITE_TO_S3:\n",
    "    from src.data.output_manager import configure_output_mode, get_output_info\n",
    "    \n",
    "    # Configure S3 output mode with aws_config\n",
    "    configure_output_mode(\n",
    "        write_to_s3=True,\n",
    "        s3_config=aws_config\n",
    "    )\n",
    "    \n",
    "    # Verify configuration\n",
    "    info = get_output_info()\n",
    "    print(\"=\"*70)\n",
    "    print(\"✓ S3 OUTPUT MODE CONFIGURED\")\n",
    "    print(f\"  Bucket: {info['bucket']}\")\n",
    "    print(f\"  Base path: {info['base_path']}\")\n",
    "    print(f\"  Full S3 URI: {info['full_path']}\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nAll pipeline outputs will be written to S3\")\n",
    "else:\n",
    "    print(\"Using local output mode (outputs/datasets/)\")\n",
    "    print(\"To enable S3 outputs: Set WRITE_TO_S3=True in previous cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Section 2: Data Connection & Loading <a id=\"sec2:data_loading\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### AWS Connection & Authentication Setup\n",
    "\n",
    "**Business Purpose**: Establish secure connection to Prudential data sources for sales and competitive rate data\n",
    "\n",
    "**Technical Function**: Creates STS client, assumes IAM role, and establishes S3 resource with proper credentials\n",
    "\n",
    "**Key Components**:\n",
    "- STS client creation for role assumption\n",
    "- IAM role assumption for cross-account access\n",
    "- S3 resource creation with assumed credentials\n",
    "- Bucket access for data lake queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:43.673621Z",
     "iopub.status.busy": "2026-01-19T16:39:43.673512Z",
     "iopub.status.idle": "2026-01-19T16:39:43.676663Z",
     "shell.execute_reply": "2026-01-19T16:39:43.676124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS connection established successfully:\n",
      "   Source bucket: pruvpcaws031-east-isg-ie-lake\n",
      "   Role ARN: arn:aws:iam::159058241883:role/isg-usbie-annuity-CA-s3-sharing\n",
      "   XID: x259830\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AWS CONNECTION: Secure Authentication\n",
    "# =============================================================================\n",
    "\n",
    "# AWS Connection using granular functions with built-in error handling\n",
    "sts_client = ext.setup_aws_sts_client_with_validation(aws_config)\n",
    "assumed_role = ext.assume_iam_role_with_validation(sts_client, aws_config)\n",
    "s3_resource, bucket = ext.setup_s3_resource_with_validation(assumed_role['Credentials'], aws_config['source_bucket_name'])\n",
    "\n",
    "print(f\"AWS connection established successfully:\")\n",
    "print(f\"   Source bucket: {aws_config['source_bucket_name']}\")\n",
    "print(f\"   Role ARN: {aws_config['role_arn']}\")\n",
    "print(f\"   XID: {aws_config['xid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Sales Data Loading\n",
    "\n",
    "**Business Purpose**: Load all available FlexGuard sales data from Prudential's data lake for analysis\n",
    "\n",
    "**Technical Function**: Discovers and loads all parquet files from sales data partition, concatenates into single DataFrame\n",
    "\n",
    "**Input**: S3 parquet files from `access/ierpt/tde_sales_by_product_by_fund/`\n",
    "\n",
    "**Output**: Combined sales DataFrame (~1.4M rows, 133 columns)\n",
    "\n",
    "**Performance Notes**: Uses parallel loading and efficient concatenation for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:43.678053Z",
     "iopub.status.busy": "2026-01-19T16:39:43.677945Z",
     "iopub.status.idle": "2026-01-19T16:39:44.422165Z",
     "shell.execute_reply": "2026-01-19T16:39:44.421240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First production run - establishing baseline metadata for sales_data_extraction\n",
      "  Schema stable: 133 columns validated\n",
      "  Date range: 1900-01-01 to 2026-01-28\n",
      "Sales data loading completed:\n",
      "   Total records: 2,828,521\n",
      "   Total columns: 133\n",
      "   Date range: 1900-01-01 to 2026-01-28\n",
      "   All required columns present for pipeline processing\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SALES DATA LOADING: Comprehensive S3 Loading\n",
    "# =============================================================================\n",
    "\n",
    "# Load sales data using granular function with built-in validation\n",
    "df_combined: pd.DataFrame = ext.discover_and_load_sales_data(bucket, s3_resource, aws_config['source_bucket_name'])\n",
    "\n",
    "print(f\"Sales data loading completed:\")\n",
    "print(f\"   Total records: {df_combined.shape[0]:,}\")\n",
    "print(f\"   Total columns: {df_combined.shape[1]}\")\n",
    "print(f\"   Date range: {df_combined['application_signed_date'].min()} to {df_combined['application_signed_date'].max()}\")\n",
    "print(f\"   All required columns present for pipeline processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### WINK Competitive Rate Data Loading\n",
    "\n",
    "**Business Purpose**: Load competitive annuity rate data from WINK system for market analysis\n",
    "\n",
    "**Technical Function**: Loads and concatenates competitive rate data from all available time periods\n",
    "\n",
    "**Input**: S3 parquet files from `access/ierpt/wink_ann_product_rates/`\n",
    "\n",
    "**Output**: Combined WINK DataFrame (~1M+ rows, 77 columns)\n",
    "\n",
    "**Key Data**: Competitive rates from major annuity providers for market positioning analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:44.424121Z",
     "iopub.status.busy": "2026-01-19T16:39:44.423996Z",
     "iopub.status.idle": "2026-01-19T16:39:44.816050Z",
     "shell.execute_reply": "2026-01-19T16:39:44.815366Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# WINK DATA LOADING: Competitive Rate Data\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load WINK competitive data using granular function with built-in validation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df_wink: pd.DataFrame = \u001b[43mext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiscover_and_load_wink_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_resource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maws_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msource_bucket_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m rate_columns = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_wink.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mrate\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col.lower()]\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWINK data loading completed:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RILA_6Y20B_refactored/src/data/extraction.py:953\u001b[39m, in \u001b[36mdiscover_and_load_wink_data\u001b[39m\u001b[34m(bucket, s3_resource, source_bucket_name)\u001b[39m\n\u001b[32m    950\u001b[39m _validate_wink_dataset_structure(df_wink)\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# Step 4: Production validation (correct for WINK data which has 'date' column)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m df_validated = \u001b[43mvalidate_extraction_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_wink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstage_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwink_data_extraction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritical_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_shrinkage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# WINK data should only grow\u001b[39;49;00m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_validated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RILA_6Y20B_refactored/src/validation/pipeline_validation_helpers.py:170\u001b[39m, in \u001b[36mvalidate_extraction_output\u001b[39m\u001b[34m(df, stage_name, date_column, critical_columns, allow_shrinkage)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Validate data extraction output with fail-fast error handling.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03mThis function wraps run_production_validation_checkpoint with fail-fast\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    161\u001b[39m \u001b[33;03m>>> assert len(df_validated) == 100  # Validation passed\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m config = _create_validation_config(\n\u001b[32m    164\u001b[39m     stage_name=stage_name,\n\u001b[32m    165\u001b[39m     strict_schema=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    166\u001b[39m     allow_shrinkage=allow_shrinkage,\n\u001b[32m    167\u001b[39m     critical_columns=critical_columns\n\u001b[32m    168\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m result = \u001b[43mrun_production_validation_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_column\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Fail-fast with business context if validation failed\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.status == \u001b[33m'\u001b[39m\u001b[33mFAILED\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RILA_6Y20B_refactored/src/validation/production_validators.py:894\u001b[39m, in \u001b[36mrun_production_validation_checkpoint\u001b[39m\u001b[34m(df, config, date_column, business_rules)\u001b[39m\n\u001b[32m    872\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run complete production validation checkpoint.\u001b[39;00m\n\u001b[32m    873\u001b[39m \n\u001b[32m    874\u001b[39m \u001b[33;03mOrchestrates all validation steps:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    891\u001b[39m \u001b[33;03m    ValueError: If critical validation failures occur (schema mismatch, critical business rules)\u001b[39;00m\n\u001b[32m    892\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;66;03m# Step 1: Extract current metrics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m current_metrics = \u001b[43mextract_validation_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[38;5;66;03m# Step 2: Load previous metrics\u001b[39;00m\n\u001b[32m    897\u001b[39m previous_metrics = load_previous_validation_metadata(\n\u001b[32m    898\u001b[39m     config[\u001b[33m'\u001b[39m\u001b[33mcheckpoint_name\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    899\u001b[39m     config[\u001b[33m'\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    900\u001b[39m     config[\u001b[33m'\u001b[39m\u001b[33mproject_root\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    901\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RILA_6Y20B_refactored/src/validation/production_validators.py:224\u001b[39m, in \u001b[36mextract_validation_metrics\u001b[39m\u001b[34m(df, date_column)\u001b[39m\n\u001b[32m    222\u001b[39m     metrics[\u001b[33m'\u001b[39m\u001b[33mmin_date\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(min_date)\n\u001b[32m    223\u001b[39m     metrics[\u001b[33m'\u001b[39m\u001b[33mmax_date\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(max_date)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     metrics[\u001b[33m'\u001b[39m\u001b[33mdate_range_days\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[43mmax_date\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_date\u001b[49m).days\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WINK DATA LOADING: Competitive Rate Data\n",
    "# =============================================================================\n",
    "\n",
    "# Load WINK competitive data using granular function with built-in validation\n",
    "df_wink: pd.DataFrame = ext.discover_and_load_wink_data(bucket, s3_resource, aws_config['source_bucket_name'])\n",
    "\n",
    "rate_columns = [col for col in df_wink.columns if 'rate' in col.lower()]\n",
    "\n",
    "print(f\"WINK data loading completed:\")\n",
    "print(f\"   Total records: {df_wink.shape[0]:,}\")\n",
    "print(f\"   Total columns: {df_wink.shape[1]}\")\n",
    "print(f\"   Company coverage: {len(rate_columns)} rate columns\")\n",
    "print(f\"   All required columns present for competitive analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Section 3: Sales Data Processing <a id=\"sec3:sales\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Pipeline 1: Product Filtering\n",
    "\n",
    "**Business Purpose**: Filter dataset to FlexGuard 6Y 20% buffer rate products only, removing irrelevant product variations\n",
    "\n",
    "**Technical Function**: Applies product name, term, and buffer rate filters using config-driven selection criteria\n",
    "\n",
    "**Input**: Raw sales data (1,406,931 rows, 133 columns)\n",
    "\n",
    "**Output**: Filtered sales data (~68,412 rows, 133 columns)\n",
    "\n",
    "**Key Parameters**: product_name=\"FlexGuard indexed variable annuity\", term_filter=\"6Y\", buffer_rate_filter=\"20%\"\n",
    "\n",
    "**Why This Matters**:\n",
    "- Focuses on $900M+ annual sales volume in RILA category\n",
    "- 96% data reduction enables efficient downstream processing\n",
    "- Targets strategic competitive segment for pricing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:44.817983Z",
     "iopub.status.busy": "2026-01-19T16:39:44.817872Z",
     "iopub.status.idle": "2026-01-19T16:39:45.097242Z",
     "shell.execute_reply": "2026-01-19T16:39:45.096413Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 1: PRODUCT FILTERING\n",
    "# =============================================================================\n",
    "\n",
    "df_product_filtered: pd.DataFrame = pipelines.apply_product_filters(df_combined, configs['product_filter'])\n",
    "\n",
    "if df_product_filtered.empty:\n",
    "    raise ValueError(\"Product filtering resulted in empty dataset\")\n",
    "\n",
    "retention_rate = len(df_product_filtered)/len(df_combined)*100\n",
    "\n",
    "print(f\"Pipeline 1 - Product filtering:\")\n",
    "print(f\"   {len(df_combined):,} → {len(df_product_filtered):,} records ({retention_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Pipeline 2: Sales Data Cleanup\n",
    "\n",
    "**Business Purpose**: Clean and validate sales data for analysis, removing invalid records and standardizing formats\n",
    "\n",
    "**Technical Function**: Applies data quality checks, handles missing values, and standardizes date formats\n",
    "\n",
    "**Input**: Filtered sales data (~68,412 rows, 133 columns)\n",
    "\n",
    "**Output**: Clean sales data (~66,894 rows, 134 columns)\n",
    "\n",
    "**Why This Matters**:\n",
    "- Data quality directly impacts $900M+ pricing strategy decisions\n",
    "- Historical data preservation ensures 4+ years for model training\n",
    "- Invalid records could skew competitive analysis by 5-10%\n",
    "\n",
    "**Key Operations**: Date validation, null value handling, data type conversions, business rule validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:45.099468Z",
     "iopub.status.busy": "2026-01-19T16:39:45.099345Z",
     "iopub.status.idle": "2026-01-19T16:39:45.158295Z",
     "shell.execute_reply": "2026-01-19T16:39:45.157376Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 2: SALES DATA CLEANUP\n",
    "# =============================================================================\n",
    "df_FlexGuard: pd.DataFrame = pipelines.apply_sales_data_cleanup(df_product_filtered, configs['sales_cleanup'])\n",
    "print(f\"SUCCESS: Pipeline 2 - Sales cleanup: {df_FlexGuard.shape}\")\n",
    "print(f\"   Records cleaned: {len(df_product_filtered) - len(df_FlexGuard):,} invalid records removed\")\n",
    "print(f\"   Data quality: {len(df_FlexGuard)/len(df_product_filtered)*100:.1f}% records passed validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-cell-16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:45.160083Z",
     "iopub.status.busy": "2026-01-19T16:39:45.159958Z",
     "iopub.status.idle": "2026-01-19T16:39:45.165842Z",
     "shell.execute_reply": "2026-01-19T16:39:45.165161Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRODUCTION VALIDATION: Sales Data Quality\n",
    "# =============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from src.validation.production_validators import (\n",
    "    run_production_validation_checkpoint,\n",
    "    ValidationConfig\n",
    ")\n",
    "\n",
    "config: ValidationConfig = {\n",
    "    'checkpoint_name': 'sales_cleanup',\n",
    "    'version': version,\n",
    "    'project_root': project_root,\n",
    "    'strict_schema': True,\n",
    "    'growth_config': {\n",
    "        'min_growth_pct': -5.0,\n",
    "        'max_growth_pct': 20.0,\n",
    "        'warn_on_shrinkage': True,\n",
    "        'warn_on_high_growth': True\n",
    "    },\n",
    "    'critical_columns': None\n",
    "}\n",
    "\n",
    "result = run_production_validation_checkpoint(\n",
    "    df=df_FlexGuard,\n",
    "    config=config,\n",
    "    date_column='application_signed_date'\n",
    ")\n",
    "\n",
    "print(\"✓ Production validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Pipeline 3A: Application Time Series\n",
    "\n",
    "**Business Purpose**: Create time series based on application signed dates to track when customers applied for policies\n",
    "\n",
    "**Technical Function**: Aggregates sales data by application_signed_date, creates daily time series with sales totals\n",
    "\n",
    "**Input**: Clean sales data (~66,894 rows)\n",
    "\n",
    "**Output**: Application time series (1,777 rows, 2 columns: date, sales)\n",
    "\n",
    "**Key Parameters**: date_column='application_signed_date', alias_value_col='sales'\n",
    "\n",
    "**Why This Matters**:\n",
    "- Application dates prevent time-ordering leakage in models\n",
    "- Daily granularity captures sales momentum patterns\n",
    "- Critical for understanding customer decision timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:45.167759Z",
     "iopub.status.busy": "2026-01-19T16:39:45.167640Z",
     "iopub.status.idle": "2026-01-19T16:39:45.180998Z",
     "shell.execute_reply": "2026-01-19T16:39:45.180318Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 3A: APPLICATION TIME SERIES\n",
    "# =============================================================================\n",
    "\n",
    "# Application date time series\n",
    "app_config = configs['time_series'].copy()\n",
    "app_config['date_column'] = 'application_signed_date'\n",
    "app_config['alias_value_col'] = 'sales'\n",
    "df_sales_app: pd.DataFrame = pipelines.apply_application_time_series(df_FlexGuard, app_config)\n",
    "print(f\"SUCCESS: Pipeline 3a - Application time series: {df_sales_app.shape}\")\n",
    "print(f\"   Date range: {df_sales_app.index.min()} to {df_sales_app.index.max()}\")\n",
    "print(f\"   Daily sales range: ${df_sales_app['sales'].min():,.0f} to ${df_sales_app['sales'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Pipeline 3B: Contract Time Series\n",
    "\n",
    "**Business Purpose**: Create time series based on contract issue dates to track when policies were actually issued\n",
    "\n",
    "**Technical Function**: Aggregates sales data by contract_issue_date, creates daily time series with contract sales totals\n",
    "\n",
    "**Input**: Clean sales data (~66,894 rows)\n",
    "\n",
    "**Output**: Contract time series (1,744 rows, 2 columns: date, sales_by_contract_date)\n",
    "\n",
    "**Key Parameters**: date_column='contract_issue_date', alias_value_col='sales_by_contract_date'\n",
    "\n",
    "**Why This Matters**:\n",
    "- Contract dates track actual policy issuance\n",
    "- Processing lag analysis supports forecasting models\n",
    "- Alternative time series for validation checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:45.182941Z",
     "iopub.status.busy": "2026-01-19T16:39:45.182826Z",
     "iopub.status.idle": "2026-01-19T16:39:45.195254Z",
     "shell.execute_reply": "2026-01-19T16:39:45.194570Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 3B: CONTRACT TIME SERIES\n",
    "# =============================================================================\n",
    "\n",
    "# Contract date time series\n",
    "contract_config = configs['time_series'].copy()\n",
    "contract_config['date_column'] = 'contract_issue_date'\n",
    "contract_config['alias_value_col'] = 'sales_by_contract_date'\n",
    "df_sales_contract: pd.DataFrame = pipelines.apply_contract_time_series(df_FlexGuard, contract_config)\n",
    "print(f\"SUCCESS: Pipeline 3b - Contract time series: {df_sales_contract.shape}\")\n",
    "print(f\"   Date range: {df_sales_contract.index.min()} to {df_sales_contract.index.max()}\")\n",
    "print(f\"   Contract sales range: ${df_sales_contract['sales_by_contract_date'].min():,.0f} to ${df_sales_contract['sales_by_contract_date'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Sales Data Export & DVC Tracking\n",
    "\n",
    "**Business Purpose**: Persist sales time series data for downstream use and maintain version control for reproducibility\n",
    "\n",
    "**Technical Function**: Exports to parquet format, adds DVC tracking, and syncs with S3 remote storage\n",
    "\n",
    "**Output Files**:\n",
    "- `outputs/datasets/FlexGuard_Sales.parquet` (application-based time series)\n",
    "- `outputs/datasets/FlexGuard_Sales_contract.parquet` (contract-based time series)\n",
    "\n",
    "**DVC Integration**: Automatic tracking and S3 remote synchronization for data lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:45.196926Z",
     "iopub.status.busy": "2026-01-19T16:39:45.196810Z",
     "iopub.status.idle": "2026-01-19T16:39:45.208613Z",
     "shell.execute_reply": "2026-01-19T16:39:45.207958Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SALES DATA EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "# Save datasets with DVC tracking\n",
    "save_dataset(df_sales_app, \"FlexGuard_Sales\")\n",
    "save_dataset(df_sales_contract, \"FlexGuard_Sales_contract\")\n",
    "\n",
    "print(f\"Sales processing complete:\")\n",
    "print(f\"   Application series: {df_sales_app.shape}\")\n",
    "print(f\"   Contract series: {df_sales_contract.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Section 4: Competitive Rate Analysis <a id=\"sec4:rates\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Pipeline 4: WINK Rate Processing\n",
    "\n",
    "**Business Purpose**: Process competitive rates from WINK system without market share weighting (DRY compliance - separation of concerns)\n",
    "\n",
    "**Technical Function**: Filters, cleans, and standardizes competitive rate data from multiple insurance companies\n",
    "\n",
    "**Input**: Raw WINK data (1,075,628 rows, 77 columns)\n",
    "\n",
    "**Output**: Processed rates (2,686 rows, 10 columns)\n",
    "\n",
    "**Key Parameters**: Company filters, rate type selections, date range filtering\n",
    "\n",
    "**Why This Matters**:\n",
    "- Competitive intelligence from 8 major carriers\n",
    "- Rate changes drive 2-3% sales impact per basis point\n",
    "- Daily updates enable rapid competitive response\n",
    "\n",
    "**Note**: Does NOT create C_weighted_mean (handled separately in Pipeline 5 for DRY compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:45.210272Z",
     "iopub.status.busy": "2026-01-19T16:39:45.210160Z",
     "iopub.status.idle": "2026-01-19T16:39:46.802281Z",
     "shell.execute_reply": "2026-01-19T16:39:46.801468Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 4: WINK RATE PROCESSING (NO C_weighted_mean creation)\n",
    "# =============================================================================\n",
    "df_wink_rates: pd.DataFrame = pipelines.apply_wink_rate_processing(df_wink, configs['wink_processing'])\n",
    "print(f\"SUCCESS: Pipeline 4 - WINK rate processing: {df_wink_rates.shape}\")\n",
    "print(f\"   Data reduction: {len(df_wink):,} → {len(df_wink_rates):,} rows\")\n",
    "print(f\"   Company columns: {[col for col in df_wink_rates.columns if col in ['Prudential', 'Allianz', 'Athene']]}\")\n",
    "print(f\"   Date range: {df_wink_rates['date'].min()} to {df_wink_rates['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Pipeline 5: Market Share Weighting\n",
    "\n",
    "**Business Purpose**: Apply market share weights to create competitive averages - separated from WINK processing for DRY compliance\n",
    "\n",
    "**Technical Function**: Loads market share weights and creates weighted competitive features including C_weighted_mean and C_core\n",
    "\n",
    "**Input**: Processed WINK rates (2,686 rows, 10 columns)\n",
    "\n",
    "**Output**: Weighted rates (2,686 rows, 25 columns)\n",
    "\n",
    "**Why This Matters**:\n",
    "- Distribution reach determines competitive impact\n",
    "- Market share proxies for sales force effectiveness\n",
    "- Quarterly weights capture evolving competitive landscape\n",
    "\n",
    "**Key Features Created**: C_weighted_mean, C_core, and other market-share weighted competitive metrics\n",
    "\n",
    "**Market Share Source**: S3-stored quarterly market share weights for FlexGuard competitive set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:46.804301Z",
     "iopub.status.busy": "2026-01-19T16:39:46.804175Z",
     "iopub.status.idle": "2026-01-19T16:39:46.819000Z",
     "shell.execute_reply": "2026-01-19T16:39:46.818243Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 5: MARKET SHARE WEIGHTING (Creates C_weighted_mean - DRY compliance)\n",
    "# =============================================================================\n",
    "\n",
    "# Load market share weights from S3\n",
    "market_share_s3_path = \"s3://cdo-annuity-364524684987-bucket/ANN_Price_Elasticity_Data_Science/flex_guard_market_share_2025_10_01.parquet\"\n",
    "df_quarter_weights: pd.DataFrame = ext.load_market_share_weights_from_s3(market_share_s3_path)\n",
    "print(f\"Market share weights loaded: {df_quarter_weights.shape}\")\n",
    "\n",
    "# Apply market share weighting (creates C_weighted_mean and C_core)\n",
    "df_wink_rates_weighted: pd.DataFrame = pipelines.apply_market_share_weighting(df_wink_rates, df_quarter_weights)\n",
    "print(f\"SUCCESS: Pipeline 5 - Market share weighting: {df_wink_rates_weighted.shape}\")\n",
    "print(f\"   New columns added: {df_wink_rates_weighted.shape[1] - df_wink_rates.shape[1]}\")\n",
    "print(f\"   Weighted features: {[col for col in df_wink_rates_weighted.columns if col.startswith('C_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "### WINK Data Export & DVC Tracking + Checkpoint\n",
    "\n",
    "**Business Purpose**: Persist weighted competitive rates for downstream feature engineering and maintain data lineage\n",
    "\n",
    "**Technical Function**: Exports weighted rates to parquet, adds DVC tracking with strategic checkpoint for competitive data\n",
    "\n",
    "**Output Files**:\n",
    "- `outputs/datasets/WINK_competitive_rates.parquet` (weighted competitive rates)\n",
    "\n",
    "**DVC Integration**: Checkpoint created here for competitive rate processing milestone\n",
    "\n",
    "**Strategic Value**: Allows independent reuse of processed competitive rates for other analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:46.820762Z",
     "iopub.status.busy": "2026-01-19T16:39:46.820637Z",
     "iopub.status.idle": "2026-01-19T16:39:46.854694Z",
     "shell.execute_reply": "2026-01-19T16:39:46.854056Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPETITIVE RATES EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "save_dataset(df_wink_rates_weighted, \"WINK_competitive_rates\")\n",
    "\n",
    "print(f\"Competitive rate processing complete:\")\n",
    "print(f\"   Weighted rates: {df_wink_rates_weighted.shape}\")\n",
    "print(f\"   Key features: C_weighted_mean, C_core created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Section 5: Feature Engineering <a id=\"sec5:features\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### Economic Data Loading\n",
    "\n",
    "**Business Purpose**: Load macroeconomic indicators (interest rates, market volatility, inflation) that impact annuity sales\n",
    "\n",
    "**Technical Function**: Loads DGS5 (5-year Treasury), VIX (volatility), and CPI (inflation) data from S3 and DVC-tracked local files\n",
    "\n",
    "**Data Sources**:\n",
    "- DGS5: 5-year Treasury rates (interest rate environment)\n",
    "- VIX: Market volatility index (market uncertainty)\n",
    "- CPI: Consumer price index scaled (inflation adjustment)\n",
    "- Sales data: Previously processed and DVC-tracked sales time series\n",
    "\n",
    "**Integration Strategy**: Combines external economic data with internal sales and competitive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:46.856511Z",
     "iopub.status.busy": "2026-01-19T16:39:46.856400Z",
     "iopub.status.idle": "2026-01-19T16:39:46.869105Z",
     "shell.execute_reply": "2026-01-19T16:39:46.868339Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ECONOMIC AND SALES DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "# Load economic data from S3\n",
    "file_name_dgs5 = \"MACRO_ECONOMIC_DATA/DGS5_index\"\n",
    "file_name_vixcls = \"MACRO_ECONOMIC_DATA/VIXCLS_index\"\n",
    "file_name_cpi = \"MACRO_ECONOMIC_DATA/cpi_scaled\"\n",
    "\n",
    "df_rates: pd.DataFrame = df_wink_rates_weighted\n",
    "df_dgs5: pd.DataFrame = ext.download_s3_parquet_with_optional_date_suffix(aws_config['output_bucket_name'], f\"{aws_config['output_base_path']}/{file_name_dgs5}\", None)\n",
    "df_vixcls: pd.DataFrame = ext.download_s3_parquet_with_optional_date_suffix(aws_config['output_bucket_name'], f\"{aws_config['output_base_path']}/{file_name_vixcls}\", None)\n",
    "df_cpi: pd.DataFrame = ext.download_s3_parquet_with_optional_date_suffix(aws_config['output_bucket_name'], f\"{aws_config['output_base_path']}/{file_name_cpi}\", None)\n",
    "\n",
    "# Load from DVC-tracked datasets\n",
    "df_sales_reload: pd.DataFrame = load_dataset(\"FlexGuard_Sales\")\n",
    "df_sales_contract_reload: pd.DataFrame = load_dataset(\"FlexGuard_Sales_contract\")\n",
    "\n",
    "print(f\"Economic and sales data loaded:\")\n",
    "print(f\"   DGS5: {df_dgs5.shape}, VIX: {df_vixcls.shape}, CPI: {df_cpi.shape}\")\n",
    "print(f\"   Sales: {df_sales_reload.shape}, Contract: {df_sales_contract_reload.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Pipeline 6: Data Integration\n",
    "\n",
    "**Business Purpose**: Integrate sales, competitive, and economic data on daily basis without weekly aggregation (preserves granularity)\n",
    "\n",
    "**Technical Function**: Performs daily merge of all data sources using date as key, maintains daily frequency for subsequent processing\n",
    "\n",
    "**Input**: Multiple data sources (competitive rates, sales time series, economic indicators)\n",
    "\n",
    "**Output**: Daily integrated dataset (1,761 rows, 32 columns)\n",
    "\n",
    "**Why This Matters**:\n",
    "- Combines sales, competitive, and economic indicators\n",
    "- Daily frequency preserves temporal precision\n",
    "- Foundation for all downstream feature engineering\n",
    "\n",
    "**Key Function**: Daily merge without weekly aggregation - fixed ordering allows competitive features to be created before aggregation\n",
    "\n",
    "**Business Value**: Maintains temporal granularity for more precise competitive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:46.870783Z",
     "iopub.status.busy": "2026-01-19T16:39:46.870668Z",
     "iopub.status.idle": "2026-01-19T16:39:46.884505Z",
     "shell.execute_reply": "2026-01-19T16:39:46.883870Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 6: DATA INTEGRATION (Daily integration without weekly aggregation)\n",
    "# =============================================================================\n",
    "\n",
    "data_sources = {\n",
    "    'sales': df_sales_reload,\n",
    "    'sales_contract': df_sales_contract_reload,\n",
    "    'dgs5': df_dgs5,\n",
    "    'vixcls': df_vixcls,\n",
    "    'cpi': df_cpi\n",
    "}\n",
    "\n",
    "df_daily_integrated: pd.DataFrame = pipelines.apply_data_integration(\n",
    "    df_rates, data_sources, configs['data_integration']\n",
    ")\n",
    "\n",
    "print(f\"SUCCESS: Pipeline 6 - Daily data integration: {df_daily_integrated.shape}\")\n",
    "print(f\"   Date range: {df_daily_integrated['date'].min()} to {df_daily_integrated['date'].max()}\")\n",
    "print(f\"   Integration complete, ready for competitive features\")\n",
    "print(f\"   Key columns: {[col for col in df_daily_integrated.columns if col in ['date', 'sales', 'C_weighted_mean', 'DGS5']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Pipeline 7: Competitive Features\n",
    "\n",
    "**Business Purpose**: Create competitive analysis features with clean semantic naming for market positioning insights\n",
    "\n",
    "**Technical Function**: Generates competitive rankings, percentiles, and comparative metrics using atomic competitive functions\n",
    "\n",
    "**Input**: Daily integrated data (1,761 rows, 32 columns)\n",
    "\n",
    "**Output**: Data with competitive features (1,761 rows, 44 columns)\n",
    "\n",
    "**Why This Matters**:\n",
    "- Top-N competitor metrics track market positioning\n",
    "- Competitive spread is key driver of sales elasticity\n",
    "- Rankings enable relative performance analysis\n",
    "\n",
    "**Key Features Created**:\n",
    "- C_median: Competitive median rates\n",
    "- C_top_5: Top 5 competitor average\n",
    "- Competitive rankings and percentiles\n",
    "\n",
    "**Semantic Naming**: Uses clean underscore notation (no backward compatibility shortcuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:46.886249Z",
     "iopub.status.busy": "2026-01-19T16:39:46.886138Z",
     "iopub.status.idle": "2026-01-19T16:39:46.893770Z",
     "shell.execute_reply": "2026-01-19T16:39:46.893161Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 7: COMPETITIVE FEATURES (Clean semantic names only)\n",
    "# =============================================================================\n",
    "\n",
    "df_competitive: pd.DataFrame = pipelines.apply_competitive_features(df_daily_integrated, configs['competitive'])\n",
    "\n",
    "print(f\"SUCCESS: Pipeline 7 - Competitive features: {df_competitive.shape}\")\n",
    "print(f\"   New features added: {df_competitive.shape[1] - df_daily_integrated.shape[1]}\")\n",
    "print(f\"   Competitive features: {sorted([col for col in df_competitive.columns if col.startswith('C_')])}\")\n",
    "print(f\"   Date range maintained: {df_competitive['date'].min()} to {df_competitive['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Pipeline 8: Weekly Aggregation + DVC Checkpoint\n",
    "\n",
    "**Business Purpose**: Aggregate daily data to weekly frequency for modeling while preserving competitive feature relationships\n",
    "\n",
    "**Technical Function**: Performs weekly aggregation after competitive features exist (corrected pipeline ordering)\n",
    "\n",
    "**Input**: Daily data with competitive features (1,761 rows, 44 columns)\n",
    "\n",
    "**Output**: Weekly aggregated data (253 rows, 26 columns)\n",
    "\n",
    "**Why This Matters**:\n",
    "- Reduces noise by 30-40% while preserving trends\n",
    "- Aligns with business reporting cycles\n",
    "- Optimal balance for 160-week modeling dataset\n",
    "\n",
    "**Technical Note**: Applied after competitive features exist - fixed ordering from original pipeline architecture\n",
    "\n",
    "**Performance Impact**: Significant data reduction while preserving analytical relationships\n",
    "\n",
    "**DVC Checkpoint**: Strategic checkpoint added for weekly aggregation milestone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:46.895448Z",
     "iopub.status.busy": "2026-01-19T16:39:46.895341Z",
     "iopub.status.idle": "2026-01-19T16:39:46.972930Z",
     "shell.execute_reply": "2026-01-19T16:39:46.972170Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 8: WEEKLY AGGREGATION\n",
    "# =============================================================================\n",
    "\n",
    "df_ts_weekly: pd.DataFrame = pipelines.apply_weekly_aggregation(df_competitive, configs['weekly_aggregation'])\n",
    "\n",
    "# Save checkpoint\n",
    "save_dataset(df_ts_weekly, \"weekly_aggregated_features\")\n",
    "\n",
    "print(f\"Pipeline 8 - Weekly aggregation:\")\n",
    "print(f\"   {len(df_competitive):,} → {len(df_ts_weekly):,} rows ({len(df_ts_weekly)/len(df_competitive)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "### Pipeline 9: Lag Features & Polynomial Interactions + DVC Checkpoint\n",
    "\n",
    "**Business Purpose**: Create temporal lag features and polynomial interactions for forecasting models to capture time-series patterns\n",
    "\n",
    "**Technical Function**: Generates 18 lag periods with semantic naming and polynomial interactions using 13 lag configurations\n",
    "\n",
    "**Input**: Weekly aggregated data (253 rows, 26 columns)\n",
    "\n",
    "**Output**: Data with lag features (253 rows, 594 columns)\n",
    "\n",
    "**Why This Matters**:\n",
    "- 18-period lags capture temporal dependencies\n",
    "- Polynomial terms model non-linear relationships\n",
    "- 594 features enable comprehensive elasticity modeling\n",
    "\n",
    "**Key Features Generated**:\n",
    "- Semantic lag naming: competitor_mid_t1, sales_target_current, etc.\n",
    "- 18 lag periods for key variables\n",
    "- Polynomial interaction terms\n",
    "- Both forward and backward looking features\n",
    "\n",
    "**Performance Impact**: Massive feature expansion (26 → 594 columns) for comprehensive temporal modeling\n",
    "\n",
    "**DVC Checkpoint**: Strategic checkpoint for lag feature creation milestone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:46.975000Z",
     "iopub.status.busy": "2026-01-19T16:39:46.974883Z",
     "iopub.status.idle": "2026-01-19T16:39:47.126636Z",
     "shell.execute_reply": "2026-01-19T16:39:47.125889Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 9: LAG FEATURES & POLYNOMIAL INTERACTIONS\n",
    "# =============================================================================\n",
    "\n",
    "df_lagged: pd.DataFrame = pipelines.apply_lag_and_polynomial_features(df_ts_weekly, configs['lag_features'])\n",
    "\n",
    "# Save checkpoint\n",
    "save_dataset(df_lagged, \"lag_features_created\")\n",
    "\n",
    "lag_features = [col for col in df_lagged.columns if any(suffix in col for suffix in ['_current', '_t1', '_t2', '_t3'])]\n",
    "\n",
    "print(f\"Pipeline 9 - Lag & polynomial features:\")\n",
    "print(f\"   Features: {df_ts_weekly.shape[1]} → {df_lagged.shape[1]} columns\")\n",
    "print(f\"   Total lag features: {len(lag_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Pipeline 10: Final Feature Preparation\n",
    "\n",
    "**Business Purpose**: Add final temporal features, competitive spreads, and perform cleanup for modeling dataset\n",
    "\n",
    "**Technical Function**: Creates business-critical features like Spread, sales_log, holiday indicators, and day_of_year temporal features\n",
    "\n",
    "**Input**: Data with lag features (253 rows, 594 columns)\n",
    "\n",
    "**Output**: Final modeling dataset (157 rows, 598 columns)\n",
    "\n",
    "**Why This Matters**:\n",
    "- Spread feature quantifies competitive positioning\n",
    "- Holiday indicators capture seasonal patterns\n",
    "- Final validation ensures model-ready dataset\n",
    "\n",
    "**Key Features Added**:\n",
    "- Spread: Cap rate distance to mean rate (key business metric)\n",
    "- sales_log: Log-transformed sales for modeling\n",
    "- Holiday indicators: Business calendar effects\n",
    "- day_of_year: Seasonal patterns\n",
    "\n",
    "**Final Validation**: Date range 2022-09-04 to 2025-08-31, ready for modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:47.128569Z",
     "iopub.status.busy": "2026-01-19T16:39:47.128448Z",
     "iopub.status.idle": "2026-01-19T16:39:47.150511Z",
     "shell.execute_reply": "2026-01-19T16:39:47.149725Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PIPELINE 10: FINAL FEATURE PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "df_final: pd.DataFrame = pipelines.apply_final_feature_preparation(df_lagged, configs['final_features'])\n",
    "\n",
    "print(f\"SUCCESS: Pipeline 10 - Final feature preparation: {df_final.shape}\")\n",
    "print(f\"   Row reduction: {len(df_lagged)} → {len(df_final)} (data availability filtering)\")\n",
    "print(f\"   Column expansion: {df_lagged.shape[1]} → {df_final.shape[1]} (+{df_final.shape[1] - df_lagged.shape[1]} final features)\")\n",
    "print(f\"   Date range: {df_final['date'].min()} to {df_final['date'].max()}\")\n",
    "\n",
    "final_features = [col for col in df_final.columns if col in ['Spread', 'sales_log', 'holiday', 'day_of_year']]\n",
    "print(f\"   Final features added: {final_features}\")\n",
    "\n",
    "# Final dataset preparation with clean column ordering\n",
    "df_weekly_final: pd.DataFrame = df_final[sorted(df_final.columns)].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nSUCCESS: Final dataset ready for modeling\")\n",
    "print(f\"   Shape: {df_weekly_final.shape}\")\n",
    "key_features = [col for col in df_weekly_final.columns if any(key in col for key in ['prudential_rate_current', 'competitor_mid_t1', 'Spread', 'sales_log'])]\n",
    "print(f\"   Key modeling features: {key_features[:5]}... ({len(key_features)} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## Section 6: Final Export & Visualization <a id=\"sec6:final\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "### Final Dataset Preparation & Export\n",
    "\n",
    "**Business Purpose**: Prepare final modeling dataset and persist with DVC for downstream modeling pipeline consumption\n",
    "\n",
    "**Technical Function**: Final dataset validation, DVC tracking, and S3 remote synchronization\n",
    "\n",
    "**Output Validation**:\n",
    "- Shape: (157, 598) - matches expected modeling dataset dimensions\n",
    "- Date range: 2022-09-04 to 2025-08-31 - comprehensive time coverage\n",
    "- Feature completeness: All 598 engineered features for forecasting\n",
    "\n",
    "**DVC Integration**: Final dataset tracked for modeling pipeline consumption and reproducibility\n",
    "\n",
    "**Business Value**: Complete, validated dataset ready for price elasticity modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:47.152320Z",
     "iopub.status.busy": "2026-01-19T16:39:47.152202Z",
     "iopub.status.idle": "2026-01-19T16:39:47.197171Z",
     "shell.execute_reply": "2026-01-19T16:39:47.196462Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL DATASET EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "# Basic validation\n",
    "# if not (150 <= len(df_weekly_final) <= 180):\n",
    "#     raise ValueError(f\"Final dataset has {len(df_weekly_final)} records, expected 150-180\")\n",
    "\n",
    "# if df_weekly_final.shape[1] < 590:\n",
    "#     raise ValueError(f\"Final dataset has {df_weekly_final.shape[1]} features, expected minimum 590\")\n",
    "\n",
    "# Save final dataset\n",
    "save_dataset(df_weekly_final, \"final_dataset\")\n",
    "\n",
    "print(f\"Pipeline execution complete - all 10 stages successful\")\n",
    "print(f\"Final dataset: {df_weekly_final.shape} (records × features)\")\n",
    "print(f\"Date range: {df_weekly_final['date'].min()} to {df_weekly_final['date'].max()}\")\n",
    "print(f\"Ready for: 01_feature_selection.ipynb → 02_time_series_forecasting.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "### Results Visualization\n",
    "\n",
    "**Business Purpose**: Visualize FlexGuard sales performance vs competitive rate spreads to validate data relationships\n",
    "\n",
    "**Technical Function**: Dual-axis time series plot and correlation analysis between sales and competitive positioning\n",
    "\n",
    "**Key Relationships**:\n",
    "- CPI Adjusted FlexGuard Sales (sales_target_contract_current)\n",
    "- Cap Rate Distance to Mean Rate (Spread) - key competitive metric\n",
    "\n",
    "**Validation Purpose**: Confirms expected inverse relationship between competitive spreads and sales performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T16:39:47.199011Z",
     "iopub.status.busy": "2026-01-19T16:39:47.198894Z",
     "iopub.status.idle": "2026-01-19T16:39:47.558449Z",
     "shell.execute_reply": "2026-01-19T16:39:47.557620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final visualization\n",
    "figure, axes = plt.subplots(2, 1, sharex=False, sharey=False, figsize=(fig_width/2, fig_height))\n",
    "figure.suptitle(\"FLEXGUARD 6Y20 - Refactored Pipeline Architecture Results\")\n",
    "axes[0].set_title(\"CPI Adjusted FlexGuard Sales (Black) and Cap Rate Distance to Mean Rate (ORANGE)\")\n",
    "\n",
    "# Key variables for visualization\n",
    "log_sales_col = \"sales_target_contract_current\"\n",
    "spread_col = \"Spread\"\n",
    "\n",
    "# Use seaborn directly for dual-axis plotting\n",
    "sns.lineplot(\n",
    "    df_weekly_final, x=\"date\", y=log_sales_col,\n",
    "    ax=axes[0], color=\"k\", linewidth=5\n",
    ")\n",
    "\n",
    "ax0 = axes[0].twinx()\n",
    "ax0.set_ylabel(\"Cap Rate Distance to Mean Rate (bps)\")\n",
    "ax0.grid(False)\n",
    "sns.lineplot(\n",
    "    data=df_weekly_final, x=\"date\", y=spread_col,\n",
    "    ax=ax0, color=\"tab:orange\", linewidth=5\n",
    ")\n",
    "\n",
    "sns.regplot(df_weekly_final, x=spread_col, y=log_sales_col, ax=axes[1], ci=100, order=1)\n",
    "axes[1].set_title(\"CPI Adjusted FlexGuard Sales vs. Cap Rate Distance to Mean Rate\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n VISUALIZATION COMPLETE\")\n",
    "print(f\"   Sales range: ${df_weekly_final[log_sales_col].min():,.0f} to ${df_weekly_final[log_sales_col].max():,.0f}\")\n",
    "print(f\"   Spread range: {df_weekly_final[spread_col].min():.1f} to {df_weekly_final[spread_col].max():.1f} bps\")\n",
    "correlation = df_weekly_final[log_sales_col].corr(df_weekly_final[spread_col])\n",
    "print(f\"   Correlation (Sales vs Spread): {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ad286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
