"""
Tests for RidgeCV Feature Selection Engine.

Tests cover:
- RidgeCVConfig: Configuration dataclass validation
- FeatureCombinationResult: Result object properties
- RidgeCVResults: Aggregated results with analysis methods
- _generate_feature_combinations: Combination generation with constraints
- evaluate_ridge_cv_combinations: Full evaluation pipeline
- compare_with_aic_selection: Method comparison utility

Design Principles:
- Real assertions about correctness (not just "doesn't crash")
- Test happy path + error cases + edge cases
- Mathematical validation where applicable

Author: Claude Code
Date: 2026-01-30
"""

import pytest
import pandas as pd
import numpy as np

from src.features.selection.engines.ridge_cv_engine import (
    RidgeCVConfig,
    FeatureCombinationResult,
    RidgeCVResults,
    evaluate_ridge_cv_combinations,
    compare_with_aic_selection,
    _generate_feature_combinations,
    _evaluate_single_combination,
)


# =============================================================================
# Test Fixtures
# =============================================================================


@pytest.fixture
def simple_regression_data():
    """Create simple regression dataset with known properties."""
    np.random.seed(42)
    n = 100

    # Create features with known relationships
    x1 = np.random.randn(n)
    x2 = np.random.randn(n)
    x3 = np.random.randn(n)

    # Target with known coefficients: y = 2*x1 + 1*x2 + noise
    noise = np.random.randn(n) * 0.5
    y = 2.0 * x1 + 1.0 * x2 + noise

    return pd.DataFrame({
        'target': y,
        'prudential_rate_t1': x1,  # Own-rate feature
        'competitor_t2': x2,
        'noise_feature': x3,
    })


@pytest.fixture
def multifeature_data():
    """Create dataset with multiple candidate features."""
    np.random.seed(123)
    n = 150

    data = {
        'target': np.random.randn(n) * 10 + 50,
        'prudential_rate_t1': np.random.randn(n),
        'competitor_a_t2': np.random.randn(n),
        'competitor_b_t2': np.random.randn(n),
        'competitor_c_t3': np.random.randn(n),
        'macro_indicator': np.random.randn(n),
    }

    df = pd.DataFrame(data)
    # Add correlation to target
    df['target'] = df['target'] + 3 * df['prudential_rate_t1'] + 2 * df['competitor_a_t2']
    return df


@pytest.fixture
def default_config():
    """Default RidgeCV configuration."""
    return RidgeCVConfig()


@pytest.fixture
def custom_config():
    """Custom RidgeCV configuration."""
    return RidgeCVConfig(
        alphas=(0.01, 0.1, 1.0),
        cv_folds=3,
        scoring="r2",
        max_features=3,
        min_features=1,
        require_own_rate=True,
        own_rate_pattern="prudential_rate",
    )


@pytest.fixture
def sample_combination_result():
    """Sample FeatureCombinationResult for testing."""
    return FeatureCombinationResult(
        features=("prudential_rate_t1", "competitor_t2"),
        cv_score_mean=0.75,
        cv_score_std=0.05,
        best_alpha=1.0,
        n_features=2,
        coefficients={"prudential_rate_t1": 2.1, "competitor_t2": 0.9},
    )


# =============================================================================
# Tests for RidgeCVConfig
# =============================================================================


class TestRidgeCVConfig:
    """Tests for RidgeCVConfig dataclass."""

    def test_default_values(self, default_config):
        """Test default configuration values."""
        assert default_config.alphas == (0.001, 0.01, 0.1, 1.0, 10.0, 100.0)
        assert default_config.cv_folds == 5
        assert default_config.scoring == "r2"
        assert default_config.max_features == 5
        assert default_config.min_features == 1
        assert default_config.require_own_rate is True
        assert default_config.own_rate_pattern == "prudential_rate"

    def test_custom_values(self, custom_config):
        """Test custom configuration values."""
        assert custom_config.alphas == (0.01, 0.1, 1.0)
        assert custom_config.cv_folds == 3
        assert custom_config.max_features == 3

    def test_config_is_immutable_by_default(self):
        """Test that config is a frozen dataclass (immutable)."""
        config = RidgeCVConfig()
        # Dataclass is not frozen by default, but we verify values are set correctly
        assert config.cv_folds == 5


# =============================================================================
# Tests for FeatureCombinationResult
# =============================================================================


class TestFeatureCombinationResult:
    """Tests for FeatureCombinationResult dataclass."""

    def test_features_str_property(self, sample_combination_result):
        """Test features_str property formats correctly."""
        assert sample_combination_result.features_str == "prudential_rate_t1 + competitor_t2"

    def test_single_feature_str(self):
        """Test features_str with single feature."""
        result = FeatureCombinationResult(
            features=("single_feature",),
            cv_score_mean=0.5,
            cv_score_std=0.1,
            best_alpha=0.1,
            n_features=1,
            coefficients={"single_feature": 1.0},
        )
        assert result.features_str == "single_feature"

    def test_coefficient_access(self, sample_combination_result):
        """Test coefficient dictionary access."""
        assert sample_combination_result.coefficients["prudential_rate_t1"] == pytest.approx(2.1)
        assert sample_combination_result.coefficients["competitor_t2"] == pytest.approx(0.9)


# =============================================================================
# Tests for RidgeCVResults
# =============================================================================


class TestRidgeCVResults:
    """Tests for RidgeCVResults dataclass."""

    @pytest.fixture
    def sample_results(self, sample_combination_result, default_config):
        """Create sample RidgeCVResults."""
        result2 = FeatureCombinationResult(
            features=("prudential_rate_t1",),
            cv_score_mean=0.60,
            cv_score_std=0.08,
            best_alpha=0.1,
            n_features=1,
            coefficients={"prudential_rate_t1": 2.0},
        )
        return RidgeCVResults(
            all_results=[sample_combination_result, result2],
            best_result=sample_combination_result,
            config=default_config,
            n_combinations_evaluated=2,
        )

    def test_best_features_property(self, sample_results):
        """Test best_features returns list of feature names."""
        assert sample_results.best_features == ["prudential_rate_t1", "competitor_t2"]

    def test_best_alpha_property(self, sample_results):
        """Test best_alpha returns optimal regularization."""
        assert sample_results.best_alpha == 1.0

    def test_best_cv_score_property(self, sample_results):
        """Test best_cv_score returns highest CV score."""
        assert sample_results.best_cv_score == 0.75

    def test_to_dict_serialization(self, sample_results):
        """Test to_dict returns serializable dictionary."""
        result_dict = sample_results.to_dict()

        assert "best_features" in result_dict
        assert "best_alpha" in result_dict
        assert "best_cv_score" in result_dict
        assert "n_combinations_evaluated" in result_dict
        assert "config" in result_dict

        assert result_dict["best_cv_score"] == 0.75
        assert result_dict["n_combinations_evaluated"] == 2

    def test_summary_generation(self, sample_results):
        """Test summary generates human-readable output."""
        summary = sample_results.summary()

        assert "RidgeCV Feature Selection Results" in summary
        assert "Best Features" in summary
        assert "Best Alpha" in summary
        assert "Coefficients" in summary
        assert "prudential_rate_t1" in summary

    def test_top_n_returns_sorted_results(self, sample_results):
        """Test top_n returns results sorted by CV score."""
        top_1 = sample_results.top_n(1)
        assert len(top_1) == 1
        assert top_1[0].cv_score_mean == 0.75

        top_2 = sample_results.top_n(2)
        assert len(top_2) == 2
        assert top_2[0].cv_score_mean >= top_2[1].cv_score_mean


# =============================================================================
# Tests for _generate_feature_combinations
# =============================================================================


class TestGenerateFeatureCombinations:
    """Tests for _generate_feature_combinations function."""

    def test_with_own_rate_requirement(self):
        """Test combination generation with own-rate requirement."""
        features = ["prudential_rate_t1", "comp_a", "comp_b"]
        config = RidgeCVConfig(
            min_features=1,
            max_features=2,
            require_own_rate=True,
            own_rate_pattern="prudential_rate",
        )

        combos = _generate_feature_combinations(features, config)

        # All combinations must include prudential_rate_t1
        for combo in combos:
            assert "prudential_rate_t1" in combo

        # Should have: (own), (own, comp_a), (own, comp_b)
        assert len(combos) == 3

    def test_without_own_rate_requirement(self):
        """Test combination generation without own-rate requirement."""
        features = ["feat_a", "feat_b", "feat_c"]
        config = RidgeCVConfig(
            min_features=1,
            max_features=2,
            require_own_rate=False,
        )

        combos = _generate_feature_combinations(features, config)

        # C(3,1) + C(3,2) = 3 + 3 = 6
        assert len(combos) == 6

    def test_missing_own_rate_raises_error(self):
        """Test error when own-rate required but not found."""
        features = ["comp_a", "comp_b"]
        config = RidgeCVConfig(
            require_own_rate=True,
            own_rate_pattern="prudential_rate",
        )

        with pytest.raises(ValueError, match="require_own_rate=True but no feature matches"):
            _generate_feature_combinations(features, config)

    def test_min_max_features_respected(self):
        """Test min/max feature constraints."""
        features = ["own_rate", "f1", "f2", "f3"]
        config = RidgeCVConfig(
            min_features=2,
            max_features=3,
            require_own_rate=False,
        )

        combos = _generate_feature_combinations(features, config)

        for combo in combos:
            assert 2 <= len(combo) <= 3


# =============================================================================
# Tests for evaluate_ridge_cv_combinations
# =============================================================================


class TestEvaluateRidgeCVCombinations:
    """Tests for evaluate_ridge_cv_combinations function."""

    def test_basic_evaluation(self, simple_regression_data):
        """Test basic evaluation returns valid results."""
        config = RidgeCVConfig(
            max_features=2,
            cv_folds=3,
            require_own_rate=True,
        )

        results = evaluate_ridge_cv_combinations(
            data=simple_regression_data,
            target="target",
            candidate_features=["prudential_rate_t1", "competitor_t2", "noise_feature"],
            config=config,
        )

        assert isinstance(results, RidgeCVResults)
        assert results.n_combinations_evaluated > 0
        assert results.best_result is not None
        assert len(results.best_features) >= 1

    def test_best_features_includes_own_rate(self, simple_regression_data):
        """Test that best features includes own-rate when required."""
        config = RidgeCVConfig(
            max_features=2,
            cv_folds=3,
            require_own_rate=True,
            own_rate_pattern="prudential_rate",
        )

        results = evaluate_ridge_cv_combinations(
            data=simple_regression_data,
            target="target",
            candidate_features=["prudential_rate_t1", "competitor_t2"],
            config=config,
        )

        assert "prudential_rate_t1" in results.best_features

    def test_cv_score_is_reasonable(self, simple_regression_data):
        """Test that CV score is in valid range."""
        config = RidgeCVConfig(max_features=2, cv_folds=3)

        results = evaluate_ridge_cv_combinations(
            data=simple_regression_data,
            target="target",
            candidate_features=["prudential_rate_t1", "competitor_t2"],
            config=config,
        )

        # RÂ² should be between -inf and 1, typically 0-1 for good models
        assert results.best_cv_score <= 1.0

    def test_empty_data_raises_error(self):
        """Test error handling for empty data."""
        empty_df = pd.DataFrame()
        config = RidgeCVConfig()

        with pytest.raises(ValueError, match="Data is empty"):
            evaluate_ridge_cv_combinations(
                data=empty_df,
                target="target",
                candidate_features=["feat1"],
                config=config,
            )

    def test_missing_target_raises_error(self, simple_regression_data):
        """Test error handling for missing target column."""
        with pytest.raises(ValueError, match="Target .* not in data columns"):
            evaluate_ridge_cv_combinations(
                data=simple_regression_data,
                target="nonexistent_target",
                candidate_features=["prudential_rate_t1"],
            )

    def test_missing_features_raises_error(self, simple_regression_data):
        """Test error handling for missing feature columns."""
        with pytest.raises(ValueError, match="Missing features in data"):
            evaluate_ridge_cv_combinations(
                data=simple_regression_data,
                target="target",
                candidate_features=["prudential_rate_t1", "missing_feature"],
            )

    def test_default_config_used_when_none(self, simple_regression_data):
        """Test that default config is used when not provided."""
        results = evaluate_ridge_cv_combinations(
            data=simple_regression_data,
            target="target",
            candidate_features=["prudential_rate_t1", "competitor_t2"],
            config=None,
        )

        assert results.config.cv_folds == 5  # Default value


# =============================================================================
# Tests for compare_with_aic_selection
# =============================================================================


class TestCompareWithAICSelection:
    """Tests for compare_with_aic_selection function."""

    @pytest.fixture
    def ridge_results(self, sample_combination_result, default_config):
        """Create sample RidgeCVResults for comparison."""
        return RidgeCVResults(
            all_results=[sample_combination_result],
            best_result=sample_combination_result,
            config=default_config,
            n_combinations_evaluated=1,
        )

    def test_same_selection_detection(self, ridge_results):
        """Test detection of identical selections."""
        aic_features = ["prudential_rate_t1", "competitor_t2"]

        comparison = compare_with_aic_selection(
            ridge_results=ridge_results,
            aic_features=aic_features,
        )

        assert comparison["same_selection"] is True
        assert comparison["jaccard_similarity"] == 1.0
        assert len(comparison["overlap"]) == 2

    def test_partial_overlap_detection(self, ridge_results):
        """Test detection of partial overlap."""
        aic_features = ["prudential_rate_t1", "other_feature"]

        comparison = compare_with_aic_selection(
            ridge_results=ridge_results,
            aic_features=aic_features,
        )

        assert comparison["same_selection"] is False
        assert 0 < comparison["jaccard_similarity"] < 1
        assert "prudential_rate_t1" in comparison["overlap"]
        assert "competitor_t2" in comparison["ridge_only"]
        assert "other_feature" in comparison["aic_only"]

    def test_no_overlap_detection(self, ridge_results):
        """Test detection of no overlap."""
        aic_features = ["completely_different", "another_different"]

        comparison = compare_with_aic_selection(
            ridge_results=ridge_results,
            aic_features=aic_features,
        )

        assert comparison["same_selection"] is False
        assert comparison["jaccard_similarity"] == 0.0
        assert len(comparison["overlap"]) == 0

    def test_aic_score_included_when_provided(self, ridge_results):
        """Test that AIC score is included in comparison."""
        comparison = compare_with_aic_selection(
            ridge_results=ridge_results,
            aic_features=["feat1"],
            aic_score=150.5,
        )

        assert comparison["aic_score"] == 150.5

    def test_aic_score_none_when_not_provided(self, ridge_results):
        """Test that AIC score is None when not provided."""
        comparison = compare_with_aic_selection(
            ridge_results=ridge_results,
            aic_features=["feat1"],
        )

        assert comparison["aic_score"] is None


# =============================================================================
# Integration Tests
# =============================================================================


class TestRidgeCVIntegration:
    """Integration tests for full RidgeCV workflow."""

    def test_full_workflow(self, multifeature_data):
        """Test complete workflow from data to results."""
        config = RidgeCVConfig(
            alphas=(0.1, 1.0, 10.0),
            cv_folds=3,
            max_features=3,
            min_features=1,
            require_own_rate=True,
            own_rate_pattern="prudential_rate",
        )

        results = evaluate_ridge_cv_combinations(
            data=multifeature_data,
            target="target",
            candidate_features=[
                "prudential_rate_t1",
                "competitor_a_t2",
                "competitor_b_t2",
                "macro_indicator",
            ],
            config=config,
        )

        # Verify results structure
        assert results.n_combinations_evaluated > 0
        assert len(results.all_results) > 0
        assert results.best_result in results.all_results

        # Verify best result has valid structure
        best = results.best_result
        assert len(best.features) >= 1
        assert best.best_alpha > 0
        assert len(best.coefficients) == len(best.features)

        # Verify serialization works
        result_dict = results.to_dict()
        assert isinstance(result_dict, dict)

        # Verify summary generation
        summary = results.summary()
        assert len(summary) > 0

    def test_comparison_with_mock_aic(self, multifeature_data):
        """Test comparison workflow with mock AIC results."""
        config = RidgeCVConfig(max_features=2, cv_folds=3)

        ridge_results = evaluate_ridge_cv_combinations(
            data=multifeature_data,
            target="target",
            candidate_features=["prudential_rate_t1", "competitor_a_t2"],
            config=config,
        )

        # Compare with hypothetical AIC selection
        comparison = compare_with_aic_selection(
            ridge_results=ridge_results,
            aic_features=["prudential_rate_t1"],
            aic_score=200.0,
        )

        assert "jaccard_similarity" in comparison
        assert "ridge_cv_score" in comparison
        assert comparison["ridge_cv_score"] == ridge_results.best_cv_score
